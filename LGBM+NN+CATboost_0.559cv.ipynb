{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load in \n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the \"../input/\" directory.\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/data-science-bowl-2019/test.csv\n",
      "/kaggle/input/data-science-bowl-2019/sample_submission.csv\n",
      "/kaggle/input/data-science-bowl-2019/train_labels.csv\n",
      "/kaggle/input/data-science-bowl-2019/specs.csv\n",
      "/kaggle/input/data-science-bowl-2019/train.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from catboost import CatBoostRegressor\n",
    "from matplotlib import pyplot\n",
    "import shap\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import gc\n",
    "import json\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_qwk_lgb_regr(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Fast cappa eval function for lgb.\n",
    "    \"\"\"\n",
    "    dist = Counter(reduce_train['accuracy_group'])\n",
    "    for k in dist:\n",
    "        dist[k] /= len(reduce_train)\n",
    "    reduce_train['accuracy_group'].hist()\n",
    "    \n",
    "    acum = 0\n",
    "    bound = {}\n",
    "    for i in range(3):\n",
    "        acum += dist[i]\n",
    "        bound[i] = np.percentile(y_pred, acum * 100)\n",
    "\n",
    "    def classify(x):\n",
    "        if x <= bound[0]:\n",
    "            return 0\n",
    "        elif x <= bound[1]:\n",
    "            return 1\n",
    "        elif x <= bound[2]:\n",
    "            return 2\n",
    "        else:\n",
    "            return 3\n",
    "\n",
    "    y_pred = np.array(list(map(classify, y_pred))).reshape(y_true.shape)\n",
    "\n",
    "    return 'cappa', cohen_kappa_score(y_true, y_pred, weights='quadratic'), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def cohenkappa(ypred, y):\n",
    "    y = y.get_label().astype(\"int\")\n",
    "    ypred = ypred.reshape((4, -1)).argmax(axis = 0)\n",
    "    loss = cohenkappascore(y, y_pred, weights = 'quadratic')\n",
    "    return \"cappa\", loss, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    print('Reading train.csv file....')\n",
    "    train = pd.read_csv('/kaggle/input/data-science-bowl-2019/train.csv')\n",
    "    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "\n",
    "    print('Reading test.csv file....')\n",
    "    test = pd.read_csv('/kaggle/input/data-science-bowl-2019/test.csv')\n",
    "    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n",
    "\n",
    "    print('Reading train_labels.csv file....')\n",
    "    train_labels = pd.read_csv('/kaggle/input/data-science-bowl-2019/train_labels.csv')\n",
    "    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n",
    "\n",
    "    print('Reading specs.csv file....')\n",
    "    specs = pd.read_csv('/kaggle/input/data-science-bowl-2019/specs.csv')\n",
    "    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n",
    "\n",
    "    print('Reading sample_submission.csv file....')\n",
    "    sample_submission = pd.read_csv('/kaggle/input/data-science-bowl-2019/sample_submission.csv')\n",
    "    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n",
    "    return train, test, train_labels, specs, sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_title(train, test, train_labels):\n",
    "    # encode title\n",
    "    train['title_event_code'] = sorted(list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code'])))\n",
    "    test['title_event_code'] = sorted(list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code'])))\n",
    "    all_title_event_code = sorted(list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique())))\n",
    "    # make a list with all the unique 'titles' from the train and test set\n",
    "    list_of_user_activities = sorted(list(set(train['title'].unique()).union(set(test['title'].unique()))))\n",
    "    # make a list with all the unique 'event_code' from the train and test set\n",
    "    list_of_event_code = sorted(list(set(train['event_code'].unique()).union(set(test['event_code'].unique()))))\n",
    "    list_of_event_id = sorted(list(set(train['event_id'].unique()).union(set(test['event_id'].unique()))))\n",
    "    # make a list with all the unique worlds from the train and test set\n",
    "    list_of_worlds = sorted(list(set(train['world'].unique()).union(set(test['world'].unique()))))\n",
    "    # create a dictionary numerating the titles\n",
    "    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n",
    "    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n",
    "    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n",
    "    assess_titles = sorted(list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index))))\n",
    "    # replace the text titles with the number titles from the dict\n",
    "    train['title'] = train['title'].map(activities_map)\n",
    "    test['title'] = test['title'].map(activities_map)\n",
    "    train['world'] = train['world'].map(activities_world)\n",
    "    test['world'] = test['world'].map(activities_world)\n",
    "    train_labels['title'] = train_labels['title'].map(activities_map)\n",
    "    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n",
    "    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n",
    "    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n",
    "    # convert text into datetime\n",
    "    train['timestamp'] = pd.to_datetime(train['timestamp'])\n",
    "    test['timestamp'] = pd.to_datetime(test['timestamp'])\n",
    "    train['hour'] = train['timestamp'].dt.hour\n",
    "    test['hour'] = test['timestamp'].dt.hour\n",
    "    train['weekday'] = train['timestamp'].dt.weekday\n",
    "    test['weekday'] = test['timestamp'].dt.weekday\n",
    "    \n",
    "    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_time = {'Welcome to Lost Lagoon!':19,'Tree Top City - Level 1':17,'Ordering Spheres':61, 'Costume Box':61,\n",
    "        '12 Monkeys':109,'Tree Top City - Level 2':25, 'Pirate\\'s Tale':80, 'Treasure Map':156,'Tree Top City - Level 3':26,\n",
    "        'Rulers':126, 'Magma Peak - Level 1':20, 'Slop Problem':60, 'Magma Peak - Level 2':22, 'Crystal Caves - Level 1':18,\n",
    "        'Balancing Act':72, 'Lifting Heavy Things':118,'Crystal Caves - Level 2':24, 'Honey Cake':142, 'Crystal Caves - Level 3':19,\n",
    "        'Heavy, Heavier, Heaviest':61}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_miss(df):\n",
    "    cnt = 0\n",
    "    for e in range(len(df)):\n",
    "        x = df['event_data'].iloc[e]\n",
    "        y = json.loads(x)['misses']\n",
    "        cnt += y\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(user_sample, test_set=False):\n",
    "    '''\n",
    "    The user_sample is a DataFrame from train or test where the only one \n",
    "    installation_id is filtered\n",
    "    And the test_set parameter is related with the labels processing, that is only requered\n",
    "    if test_set=False\n",
    "    '''\n",
    "    # Constants and parameters declaration\n",
    "    last_activity = 0\n",
    "    games_count = 0\n",
    "    last_game_acc = 0\n",
    "    accumulated_game_acc = 0\n",
    "    forget_preserve_game_acc = 0\n",
    "    \n",
    "    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "    Assessment_mean_event_count = 0\n",
    "    Game_mean_event_count = 0\n",
    "    Activity_mean_event_count = 0\n",
    "    mean_game_round = 0\n",
    "    mean_game_duration = 0 \n",
    "    mean_game_level = 0\n",
    "    accumulated_game_miss = 0\n",
    "    \n",
    "    # new features: time spent in each activity\n",
    "    last_session_time_sec = 0\n",
    "    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n",
    "    all_assessments = []\n",
    "    test_train_assessment = []\n",
    "    accumulated_accuracy_group = 0\n",
    "    accumulated_accuracy = 0\n",
    "    accumulated_correct_attempts = 0 \n",
    "    accumulated_uncorrect_attempts = 0\n",
    "    accumulated_actions = 0\n",
    "    counter = 0\n",
    "    time_first_activity = float(user_sample['timestamp'].values[0])\n",
    "    durations = []\n",
    "    clip_durations = []\n",
    "    Activity_durations = []\n",
    "    Game_durations = []\n",
    "    last_accuracy_title = {'acc_' + title: -1 for title in assess_titles}\n",
    "    accumulated_accuracy_title = {'accumulated_acc_' + title: 0 for title in assess_titles} ##################################3\n",
    "    accumulated_title = {'accumulated_' + title: 0 for title in assess_titles} #####################################\n",
    "    event_code_count: Dict[str, int] = {ev: 0 for ev in list_of_event_code}\n",
    "    event_id_count: Dict[str, int] = {eve: 0 for eve in list_of_event_id}\n",
    "    title_count: Dict[str, int] = {eve: 0 for eve in activities_labels.values()} \n",
    "    title_event_code_count: Dict[str, int] = {t_eve: 0 for t_eve in all_title_event_code}\n",
    "        \n",
    "    # last features\n",
    "    sessions_count = 0\n",
    "    \n",
    "    # itarates through each session of one instalation_id\n",
    "    for i, session in user_sample.groupby('game_session', sort=False):\n",
    "        # i = game_session_id\n",
    "        # session is a DataFrame that contain only one game_session\n",
    "        \n",
    "        # get some sessions information\n",
    "        session_type = session['type'].iloc[0]\n",
    "        session_title = session['title'].iloc[0]\n",
    "        session_title_text = activities_labels[session_title]\n",
    "        \n",
    "        if session_type == 'Clip':\n",
    "            clip_durations.append((clip_time[activities_labels[session_title]]))\n",
    "        \n",
    "        if session_type == 'Activity':\n",
    "            Activity_durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            Activity_mean_event_count = (Activity_mean_event_count + session['event_count'].iloc[-1])/2.0\n",
    "        \n",
    "        if session_type == 'Game':\n",
    "            Game_durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            Game_mean_event_count = (Game_mean_event_count + session['event_count'].iloc[-1])/2.0\n",
    "            \n",
    "            game_s = session[session.event_code == 2030]   \n",
    "            misses_cnt = cnt_miss(game_s)\n",
    "            accumulated_game_miss += misses_cnt\n",
    "\n",
    "            ############## 添加的game的准确度特征 #####################\n",
    "            games = session.query(f'event_code == {4020}')\n",
    "            game_correct = games['event_data'].str.contains('true').sum()\n",
    "            game_uncorrect = games['event_data'].str.contains('false').sum()\n",
    "            if game_correct + game_uncorrect > 0:\n",
    "                games_count = games_count + 1\n",
    "            last_game_acc = game_correct / (game_correct + game_uncorrect) if game_correct + game_uncorrect > 0 else 0\n",
    "            forget_preserve_game_acc = (accumulated_game_acc + last_game_acc) / 2\n",
    "            accumulated_game_acc = accumulated_game_acc + last_game_acc\n",
    "            # games_count = games_count + 1\n",
    "            ########################################################\n",
    "            \n",
    "            try:\n",
    "                game_round = json.loads(session['event_data'].iloc[-1])[\"round\"]\n",
    "                mean_game_round =  (mean_game_round + game_round)/2.0\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                game_duration = json.loads(session['event_data'].iloc[-1])[\"duration\"]\n",
    "                mean_game_duration = (mean_game_duration + game_duration) /2.0\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                game_level = json.loads(session['event_data'].iloc[-1])[\"level\"]\n",
    "                mean_game_level = (mean_game_level + game_level) /2.0\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        # for each assessment, and only this kind off session, the features below are processed\n",
    "        # and a register are generated\n",
    "        if (session_type == 'Assessment') & (test_set or len(session)>1):\n",
    "            # search for event_code 4100, that represents the assessments trial\n",
    "            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n",
    "            # then, check the numbers of wins and the number of losses\n",
    "            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n",
    "            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n",
    "            # copy a dict to use as feature template, it's initialized with some itens: \n",
    "            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n",
    "            features = user_activities_count.copy()\n",
    "            features.update(last_accuracy_title.copy())\n",
    "            features.update(event_code_count.copy())\n",
    "            features.update(event_id_count.copy())\n",
    "            features.update(title_count.copy())\n",
    "            features.update(title_event_code_count.copy())\n",
    "#             features.update(accumulated_accuracy_title.copy()) ###############################3\n",
    "#             features.update(last_accuracy_title.copy())\n",
    "#             features.update(last_accuracy_title.copy())\n",
    "            features['last_game_acc'] = last_game_acc\n",
    "            features['accumulated_game_acc'] = accumulated_game_acc / games_count if games_count > 0 else 0\n",
    "            features['forget_preserve_game_acc'] = forget_preserve_game_acc\n",
    "            features['installation_session_count'] = sessions_count\n",
    "            # #####################\n",
    "#             features['hour'] = session['hour'].iloc[-1]\n",
    "#             # features['weekday'] = session['weekday'].iloc[-1]\n",
    "#             features['Assessment_mean_event_count'] = Assessment_mean_event_count\n",
    "#             features['Game_mean_event_count'] = Game_mean_event_count\n",
    "#             features['Activity_mean_event_count'] = Activity_mean_event_count\n",
    "#             features['mean_game_round'] = mean_game_round\n",
    "#             features['mean_game_duration'] = mean_game_duration\n",
    "#             features['mean_game_level'] = mean_game_level\n",
    "#             features['accumulated_game_miss'] = accumulated_game_miss\n",
    "            \n",
    "            variety_features = [('var_event_code', event_code_count),\n",
    "                              ('var_event_id', event_id_count),\n",
    "                               ('var_title', title_count),\n",
    "                               ('var_title_event_code', title_event_code_count)]\n",
    "            \n",
    "            for name, dict_counts in variety_features:\n",
    "                arr = np.array(list(dict_counts.values()))\n",
    "                features[name] = np.count_nonzero(arr)\n",
    "                 \n",
    "            # get installation_id for aggregated features\n",
    "            features['installation_id'] = session['installation_id'].iloc[-1]\n",
    "            # add title as feature, remembering that title represents the name of the game\n",
    "            features['session_title'] = session['title'].iloc[0]\n",
    "            # the 4 lines below add the feature of the history of the trials of this player\n",
    "            # this is based on the all time attempts so far, at the moment of this assessment\n",
    "            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n",
    "            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n",
    "            accumulated_correct_attempts += true_attempts \n",
    "            accumulated_uncorrect_attempts += false_attempts\n",
    "            # the time spent in the app so far\n",
    "            if durations == []:\n",
    "                features['duration_mean'] = 0\n",
    "                features['duration_std'] = 0\n",
    "            else:\n",
    "                features['duration_mean'] = np.mean(durations)\n",
    "                features['duration_std'] = np.std(durations)\n",
    "#             if clip_durations == []:\n",
    "#                 features['Clip_duration_mean'] = 0\n",
    "#                 features['Clip_duration_std'] = 0\n",
    "#             else:\n",
    "#                 features['Clip_duration_mean'] = np.mean(clip_durations)\n",
    "#                 features['Clip_duration_std'] = np.std(clip_durations)\n",
    "                \n",
    "#             if Activity_durations == []:\n",
    "#                 features['Activity_duration_mean'] = 0\n",
    "#                 features['Activity_duration_std'] = 0\n",
    "#             else:\n",
    "#                 features['Activity_duration_mean'] = np.mean(Activity_durations)\n",
    "#                 features['Activity_duration_std'] = np.std(Activity_durations)\n",
    "                \n",
    "#             if Game_durations == []:\n",
    "#                 features['Game_duration_mean'] = 0\n",
    "#                 features['Game_duration_std'] = 0\n",
    "#             else:\n",
    "#                 features['Game_duration_mean'] = np.mean(Game_durations)\n",
    "#                 features['Game_duration_std'] = np.std(Game_durations)\n",
    "            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n",
    "            # the accurace is the all time wins divided by the all time attempts\n",
    "            features['accumulated_accuracy'] = accumulated_accuracy/counter if counter > 0 else 0\n",
    "            accuracy = true_attempts/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n",
    "            accumulated_accuracy += accuracy\n",
    "            last_accuracy_title['acc_' + session_title_text] = accuracy\n",
    "            # a feature of the current accuracy categorized\n",
    "            # it is a counter of how many times this player was in each accuracy group\n",
    "            #########计算每个title的准确率#########\n",
    "            if (true_attempts+false_attempts) != 0:\n",
    "                accumulated_title['accumulated_' + session_title_text] += 1\n",
    "                accumulated_accuracy_title['accumulated_acc_' + session_title_text] += last_accuracy_title['acc_' + session_title_text]\n",
    "                accumulated_accuracy_title['accumulated_acc_' + session_title_text] = accumulated_accuracy_title['accumulated_acc_' + session_title_text] / accumulated_title['accumulated_' + session_title_text]\n",
    "            #########计算每个title的准确率#########\n",
    "            if accuracy == 0:\n",
    "                features['accuracy_group'] = 0\n",
    "            elif accuracy == 1:\n",
    "                features['accuracy_group'] = 3\n",
    "            elif accuracy == 0.5:\n",
    "                features['accuracy_group'] = 2\n",
    "            else:\n",
    "                features['accuracy_group'] = 1\n",
    "            features.update(accuracy_groups)\n",
    "            accuracy_groups[features['accuracy_group']] += 1\n",
    "            # mean of the all accuracy groups of this player\n",
    "            features['accumulated_accuracy_group'] = accumulated_accuracy_group/counter if counter > 0 else 0\n",
    "            accumulated_accuracy_group += features['accuracy_group']\n",
    "            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n",
    "            features['accumulated_actions'] = accumulated_actions\n",
    "            \n",
    "            # there are some conditions to allow this features to be inserted in the datasets\n",
    "            # if it's a test set, all sessions belong to the final dataset\n",
    "            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n",
    "            # that means, must exist an event_code 4100 or 4110\n",
    "            if test_set:\n",
    "                all_assessments.append(features)\n",
    "                if true_attempts+false_attempts > 0:\n",
    "                    test_train_assessment.append(features)\n",
    "            elif true_attempts+false_attempts > 0:\n",
    "                all_assessments.append(features)\n",
    "                \n",
    "            counter += 1\n",
    "        \n",
    "        sessions_count += 1\n",
    "        # this piece counts how many actions was made in each event_code so far\n",
    "        def update_counters(counter: dict, col: str):\n",
    "                num_of_session_count = Counter(session[col])\n",
    "                for k in num_of_session_count.keys():\n",
    "                    x = k\n",
    "                    if col == 'title':\n",
    "                        x = activities_labels[k]\n",
    "                    counter[x] += num_of_session_count[k]\n",
    "                return counter\n",
    "            \n",
    "        event_code_count = update_counters(event_code_count, \"event_code\")\n",
    "        event_id_count = update_counters(event_id_count, \"event_id\")\n",
    "        title_count = update_counters(title_count, 'title')\n",
    "        title_event_code_count = update_counters(title_event_code_count, 'title_event_code')\n",
    "\n",
    "        # counts how many actions the player has done so far, used in the feature of the same name\n",
    "        accumulated_actions += len(session)\n",
    "        if last_activity != session_type:\n",
    "            user_activities_count[session_type] += 1\n",
    "            last_activitiy = session_type \n",
    "                        \n",
    "    # if it't the test_set, only the last assessment must be predicted, the previous are scraped\n",
    "    if test_set:\n",
    "        return test_train_assessment, all_assessments[-1]\n",
    "    # in the train_set, all assessments goes to the dataset\n",
    "    return all_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_test(train, test):\n",
    "    compiled_train = []\n",
    "    compiled_test = []\n",
    "    compiled_train_test = []\n",
    "    for i, (ins_id, user_sample) in tqdm(enumerate(train.groupby('installation_id', sort = False)), total = 17000):\n",
    "        compiled_train += get_data(user_sample)\n",
    "    for ins_id, user_sample in tqdm(test.groupby('installation_id', sort = False), total = 1000):\n",
    "        test_train, test_data = get_data(user_sample, test_set = True)\n",
    "        compiled_train += test_train\n",
    "        compiled_train_test += test_train\n",
    "        compiled_test.append(test_data)\n",
    "    reduce_train = pd.DataFrame(compiled_train)\n",
    "    reduce_test = pd.DataFrame(compiled_test)\n",
    "    reduce_train_test = pd.DataFrame(compiled_train_test)\n",
    "    categoricals = ['session_title']\n",
    "    reduce_train.shape\n",
    "    return reduce_train, reduce_test, reduce_train_test, categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Model(object):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.features = features\n",
    "        self.n_splits = n_splits\n",
    "        self.categoricals = categoricals\n",
    "        self.target = 'accuracy_group'\n",
    "        self.cv = self.get_cv()\n",
    "        self.verbose = verbose\n",
    "        self.params = self.get_params()\n",
    "        self.y_pred, self.score, self.model = self.fit()\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_cv(self):\n",
    "        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        return cv.split(self.train_df, self.train_df[self.target])\n",
    "    \n",
    "    def get_params(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "        \n",
    "    def fit(self):\n",
    "        oof_pred = np.zeros((len(reduce_train), ))\n",
    "        y_pred = np.zeros((len(reduce_test), ))\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv):\n",
    "            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n",
    "            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n",
    "            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "            model = self.train_model(train_set, val_set)\n",
    "            conv_x_val = self.convert_x(x_val)\n",
    "            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)\n",
    "            x_test = self.convert_x(self.test_df[self.features])\n",
    "            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "            print('Partial score of fold {} is: {}'.format(fold, eval_qwk_lgb_regr(y_val, oof_pred[val_idx])[1]))\n",
    "        _, loss_score, _ = eval_qwk_lgb_regr(self.train_df[self.target], oof_pred)\n",
    "        if self.verbose:\n",
    "            print('Our oof cohen kappa score is: ', loss_score)\n",
    "        return y_pred, loss_score, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lgb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n",
    "        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'n_estimators':5000,\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'regression',\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.75,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'feature_fraction': 0.9,\n",
    "                    'random_state':42,\n",
    "                    'max_depth': 15,\n",
    "                    'lambda_l1': 1,  \n",
    "                    'lambda_l2': 1,\n",
    "                    'early_stopping_rounds': 100\n",
    "                    }\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xgb_Model(Base_Model):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        features_xgb = features.copy()\n",
    "        all_train_test = pd.concat([train_df, test_df], axis=0)\n",
    "        all_train_test = pd.get_dummies(all_train_test, prefix = [\"session_title\"], columns = [\"session_title\"])\n",
    "        train_df1 = all_train_test[0:train_df.shape[0]]\n",
    "        test_df1 = all_train_test[train_df.shape[0]:]\n",
    "        features_xgb = features_xgb + [\"session_title_4\", \"session_title_8\", \"session_title_9\", \"session_title_10\", \"session_title_30\"]\n",
    "        features_xgb.remove('session_title')\n",
    "        super().__init__(train_df1, test_df1, features_xgb, categoricals, n_splits, verbose)\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return xgb.train(self.params, train_set, \n",
    "                         num_boost_round=5000, evals=[(train_set, 'train'), (val_set, 'val')], \n",
    "                         verbose_eval=verbosity, early_stopping_rounds=100)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = xgb.DMatrix(x_train, y_train)\n",
    "        val_set = xgb.DMatrix(x_val, y_val)\n",
    "        return train_set, val_set\n",
    "    \n",
    "    def convert_x(self, x):\n",
    "        return xgb.DMatrix(x)\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'colsample_bytree': 0.8,                 \n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth': 10,\n",
    "            'subsample': 1,\n",
    "            'objective':'reg:squarederror',\n",
    "            #'eval_metric':'rmse',\n",
    "            'min_child_weight':3,\n",
    "            'random_state':42,\n",
    "            'gamma':0.25,\n",
    "            'n_estimators':5000}\n",
    "\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        clf = CatBoostRegressor(**self.params)\n",
    "        clf.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                eval_set=(val_set['X'], val_set['y']),\n",
    "                verbose=verbosity, \n",
    "                cat_features=self.categoricals)\n",
    "        return clf\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'loss_function': 'RMSE',\n",
    "                   'task_type': \"CPU\",\n",
    "                   'iterations': 5000,\n",
    "                   'od_type': \"Iter\",\n",
    "                    'depth': 10,\n",
    "                  'colsample_bylevel': 0.5, \n",
    "                   'early_stopping_rounds': 300,\n",
    "                    'l2_leaf_reg': 18,\n",
    "                   'random_seed': 42,\n",
    "                    'use_best_model': True\n",
    "                    }\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "class Nn_Model(Base_Model):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        features = features.copy()\n",
    "        if len(categoricals) > 0:\n",
    "            for cat in categoricals:\n",
    "                enc = OneHotEncoder()\n",
    "                train_cats = enc.fit_transform(train_df[[cat]])\n",
    "                test_cats = enc.transform(test_df[[cat]])\n",
    "                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "                features += cat_cols\n",
    "                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "                train_df = pd.concat([train_df, train_cats], axis=1)\n",
    "                test_df = pd.concat([test_df, test_cats], axis=1)\n",
    "        scalar = MinMaxScaler()\n",
    "        train_df[features] = scalar.fit_transform(train_df[features])\n",
    "        test_df[features] = scalar.transform(test_df[features])\n",
    "        print(train_df[features].shape)\n",
    "        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(train_set['X'].shape[1],)),\n",
    "            tf.keras.layers.Dense(200, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(100, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(25, activation='relu'),\n",
    "            tf.keras.layers.LayerNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4), loss='mse')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                validation_data=(val_set['X'], val_set['y']),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "class Cnn_Model(Base_Model):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        features = features.copy()\n",
    "        if len(categoricals) > 0:\n",
    "            for cat in categoricals:\n",
    "                enc = OneHotEncoder()\n",
    "                train_cats = enc.fit_transform(train_df[[cat]])\n",
    "                test_cats = enc.transform(test_df[[cat]])\n",
    "                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "                features += cat_cols\n",
    "                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "                train_df = pd.concat([train_df, train_cats], axis=1)\n",
    "                test_df = pd.concat([test_df, test_cats], axis=1)\n",
    "        scalar = MinMaxScaler()\n",
    "        train_df[features] = scalar.fit_transform(train_df[features])\n",
    "        test_df[features] = scalar.transform(test_df[features])\n",
    "        self.create_feat_2d(features)\n",
    "        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n",
    "        \n",
    "    def create_feat_2d(self, features, n_feats_repeat=50):\n",
    "        self.n_feats = len(features)\n",
    "        self.n_feats_repeat = n_feats_repeat\n",
    "        self.mask = np.zeros((self.n_feats_repeat, self.n_feats), dtype=np.int32)\n",
    "        for i in range(self.n_feats_repeat):\n",
    "            l = list(range(self.n_feats))\n",
    "            for j in range(self.n_feats):\n",
    "                c = l.pop(choice(range(len(l))))\n",
    "                self.mask[i, j] = c\n",
    "        self.mask = tf.convert_to_tensor(self.mask)\n",
    "        print(self.mask.shape)\n",
    "       \n",
    "        \n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "\n",
    "        inp = tf.keras.layers.Input(shape=(self.n_feats))\n",
    "        x = tf.keras.layers.Lambda(lambda x: tf.gather(x, self.mask, axis=1))(inp)\n",
    "        x = tf.keras.layers.Reshape((self.n_feats_repeat, self.n_feats, 1))(x)\n",
    "        x = tf.keras.layers.Conv2D(18, (50, 50), strides=50, activation='relu')(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        #x = tf.keras.layers.Dense(200, activation='relu')(x)\n",
    "        #x = tf.keras.layers.LayerNormalization()(x)\n",
    "        #x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(50, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        out = tf.keras.layers.Dense(1)(x)\n",
    "        \n",
    "        model = tf.keras.Model(inp, out)\n",
    "    \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='mse')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                validation_data=(val_set['X'], val_set['y']),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        return model\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train.csv file....\n",
      "Training.csv file have 11341042 rows and 11 columns\n",
      "Reading test.csv file....\n",
      "Test.csv file have 1156414 rows and 11 columns\n",
      "Reading train_labels.csv file....\n",
      "Train_labels.csv file have 17690 rows and 7 columns\n",
      "Reading specs.csv file....\n",
      "Specs.csv file have 386 rows and 3 columns\n",
      "Reading sample_submission.csv file....\n",
      "Sample_submission.csv file have 1000 rows and 2 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c41fc94930401bab147a0389808cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=17000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e80e368dc6741ebaa5a4212e757fff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "train, test, train_labels, specs, sample_submission = read_data()\n",
    "# get usefull dict with maping encode\n",
    "train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)\n",
    "# tranform function to get the train and test set\n",
    "reduce_train, reduce_test, reduce_train_test, categoricals = get_train_and_test(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_train.columns]\n",
    "reduce_test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in reduce_test.columns]\n",
    "def stract_hists(feature, train=reduce_train, test=reduce_test, adjust=False, plot=False):\n",
    "    n_bins = 10\n",
    "    train_data = train[feature]\n",
    "    test_data = test[feature]\n",
    "    if adjust:\n",
    "        test_data *= train_data.mean() / test_data.mean()\n",
    "    perc_90 = np.percentile(train_data, 95)\n",
    "    train_data = np.clip(train_data, 0, perc_90)\n",
    "    test_data = np.clip(test_data, 0, perc_90)\n",
    "    train_hist = np.histogram(train_data, bins=n_bins)[0] / len(train_data)\n",
    "    test_hist = np.histogram(test_data, bins=n_bins)[0] / len(test_data)\n",
    "    msre = mean_squared_error(train_hist, test_hist)\n",
    "    if plot:\n",
    "        print(msre)\n",
    "        plt.bar(range(n_bins), train_hist, color='blue', alpha=0.5)\n",
    "        plt.bar(range(n_bins), test_hist, color='red', alpha=0.5)\n",
    "        plt.show()\n",
    "    return msre\n",
    "# stract_hists('Magma Peak - Level 1_2000', adjust=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n",
    "features = [x for x in features if x not in ['accuracy_group', 'installation_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['27253bdc', '2040', '37c53127', 'dcaede90', '26fd2d99', '08fd73f3', '2b9272f4', '73757a5e', 'Scrub_A_Dub_3021', 'Scrub_A_Dub_2050', 'Scrub_A_Dub_2030', 'Scrub_A_Dub_2020', 'Scrub_A_Dub_2040', 'Scrub_A_Dub_3121', '4235', '85de926c', 'ad148f58', 'Bubble_Bath_4235', 'Bubble_Bath_4230', '5010', '71e712d8', 'a6d66e51', 'Watering_Hole__Activity__5010', 'Watering_Hole__Activity__5000', '3010', '3020', '3021', '1996c610', 'Dino_Drink_4031', 'installation_session_count', 'a1192f43', 'Crystals_Rule_4050', '2030', '1340b8d7', 'Bubble_Bath_4220', 'df4fe8b6', 'Chest_Sorter__Assessment__3020', 'Chest_Sorter__Assessment__3120', 'Flower_Waterer__Activity__4090', 'd06f75b5', '1beb320a', '8f094001', '895865f3', 'c54cf6c5', 'Bubble_Bath_2025', 'Bubble_Bath_4020', 'Bubble_Bath_4045', 'Bubble_Bath_2035', 'Bubble_Bath_2020', 'Bubble_Bath_2030', 'Bird_Measurer__Assessment__4030', 'Bird_Measurer__Assessment__2000', 'Sandcastle_Builder__Activity__4020', '38074c54', '3afb49e6', '222660ff', 'Chest_Sorter__Assessment__3021', 'Chest_Sorter__Assessment__2030', 'Chest_Sorter__Assessment__2010', 'Chest_Sorter__Assessment__3121', '9e4c8c7b', 'All_Star_Sorting_3010', 'All_Star_Sorting_3110', 'Egg_Dropper__Activity__4070', 'Leaf_Leader_2075', 'Bird_Measurer__Assessment__2030', 'Cart_Balancer__Assessment__3110', '15eb4a7d', 'Bubble_Bath_3010', 'Bubble_Bath_3110', 'Leaf_Leader_4090', 'Scrub_A_Dub_4070', 'Chest_Sorter__Assessment__4070', 'Bubble_Bath_2080', 'All_Star_Sorting_4090', 'Scrub_A_Dub_2080', 'Sandcastle_Builder__Activity__4021', '16dffff1', '77ead60d', 'Dino_Drink_3121', 'Dino_Drink_3021', 'Dino_Drink_2030', 'Cauldron_Filler__Assessment__2010', 'Chest_Sorter__Assessment__4040', 'Flower_Waterer__Activity__4022', '28a4eb9a', 'Dino_Dive_3020', 'Dino_Dive_3120', 'Happy_Camel_2083', 'Fireworks__Activity__2000', 'Bug_Measurer__Activity__2000', 'All_Star_Sorting_4095', '1c178d24', 'cf7638f3', 'a592d54e', 'Pan_Balance_3121', 'Pan_Balance_2030', 'Pan_Balance_3021', 'Pan_Balance_2020', 'beb0a7b9', 'Fireworks__Activity__3110', 'Fireworks__Activity__3010', 'Chicken_Balancer__Activity__4080', 'd88e8f25', 'Scrub_A_Dub_3020', 'Scrub_A_Dub_3120', '58a0de5c', 'f5b8c21a', 'Air_Show_3021', 'Air_Show_3121', 'Air_Show_2030', 'e5734469', 'Dino_Drink_3120', 'Dino_Drink_3020', 'Chow_Time_4090', 'Happy_Camel_4045', 'Pan_Balance_4035', 'Chicken_Balancer__Activity__4070', 'Mushroom_Sorter__Assessment__4070', 'Fireworks__Activity__4030', 'Dino_Dive_4090', 'Pan_Balance_4100', '1bb5fbdb', 'Sandcastle_Builder__Activity__3010', 'Sandcastle_Builder__Activity__3110', '5154fc30', '3babcb9b', '3ddc79c3', '3323d7e9', 'e720d930', 'Crystals_Rule_3110', 'Crystals_Rule_2020', 'Crystals_Rule_2030', 'Crystals_Rule_3121', 'Crystals_Rule_3021', 'Crystals_Rule_3010', 'Chicken_Balancer__Activity__4020', '923afab1', 'Cauldron_Filler__Assessment__3010', 'Cauldron_Filler__Assessment__3110', 'Bug_Measurer__Activity__4035', 'Egg_Dropper__Activity__4025', '1375ccb7', 'Bird_Measurer__Assessment__3010', 'Bird_Measurer__Assessment__3110', 'Bubble_Bath_4080', 'Cauldron_Filler__Assessment__4090', 'Fireworks__Activity__4070', 'Leaf_Leader_4020', 'Pan_Balance_4010', '36fa3ebe', 'c7fe2a55', 'Happy_Camel_2030', 'Happy_Camel_3121', 'Happy_Camel_3021', 'Leaf_Leader_4010', 'Pan_Balance_4090', 'Cart_Balancer__Assessment__4070', 'All_Star_Sorting_4080', '9b23e8ee', 'Egg_Dropper__Activity__2000', 'Egg_Dropper__Activity__2020', 'Dino_Drink_2070', 'Fireworks__Activity__4020', 'Air_Show_4070', 'Chow_Time_4080', 'Fireworks__Activity__4080', 'c277e121', 'd45ed6a1', 'All_Star_Sorting_3120', 'All_Star_Sorting_2025', 'All_Star_Sorting_3020', 'Air_Show_4020', 'Cauldron_Filler__Assessment__2000', 'b2e5b0f1', 'b74258a0', 'Cart_Balancer__Assessment__2010', 'Cart_Balancer__Assessment__2030', 'Cart_Balancer__Assessment__3121', 'Watering_Hole__Activity__4090', '7ad3efc6', 'Cart_Balancer__Assessment__2020', 'Cart_Balancer__Assessment__2000', 'b012cd7f', 'e5c9df6f', 'Leaf_Leader_3121', 'Leaf_Leader_2030', 'Leaf_Leader_3021', 'Bottle_Filler__Activity__4090', 'All_Star_Sorting_4030', 'Cart_Balancer__Assessment__4040', 'Bubble_Bath_4070', '67439901', 'Bottle_Filler__Activity__3010', 'Bottle_Filler__Activity__3110', 'Sandcastle_Builder__Activity__4035', '7f0836bf', 'Dino_Drink_3110', 'Dino_Drink_3010', 'Chow_Time_4010', '84b0e0c8', 'Chicken_Balancer__Activity__3010', 'Chicken_Balancer__Activity__3110', 'Happy_Camel_2000', 'Dino_Dive_2060', 'Bottle_Filler__Activity__4080', 'All_Star_Sorting_2020', 'Scrub_A_Dub_2000', 'Crystals_Rule_4010', 'Cart_Balancer__Assessment__4020', 'Happy_Camel_4010', '832735e1', 'Dino_Dive_3010', 'Dino_Dive_3110', 'Leaf_Leader_4095', 'Chest_Sorter__Assessment__4080', 'Bottle_Filler__Activity__4020', '49ed92e9', 'Watering_Hole__Activity__3010', 'Watering_Hole__Activity__3110', '7ab78247', 'Egg_Dropper__Activity__3110', 'Egg_Dropper__Activity__3010', '9d29771f', 'c74f40cd', '28ed704e', '83c6c409', 'Mushroom_Sorter__Assessment__2020', 'Mushroom_Sorter__Assessment__3021', 'Mushroom_Sorter__Assessment__4025', 'Mushroom_Sorter__Assessment__2035', 'Mushroom_Sorter__Assessment__3121', '3ccd3f02', 'Chest_Sorter__Assessment__3010', 'Chest_Sorter__Assessment__3110', 'Bubble_Bath_4090', 'b5053438', '28520915', 'Cauldron_Filler__Assessment__3121', 'Cauldron_Filler__Assessment__2030', 'Cauldron_Filler__Assessment__3021', 'Dino_Drink_2020', 'Watering_Hole__Activity__2000', 'Bird_Measurer__Assessment__2020', 'e57dd7af', 'Leaf_Leader_3120', 'Leaf_Leader_3020', '155f62a4', 'Chest_Sorter__Assessment__2000', 'Chest_Sorter__Assessment__2020', 'Bubble_Bath_2083', 'bbfe0445', 'Flower_Waterer__Activity__3010', 'Flower_Waterer__Activity__3110', 'a1e4395d', 'Mushroom_Sorter__Assessment__3010', 'Mushroom_Sorter__Assessment__3110', 'Mushroom_Sorter__Assessment__4100', 'Bubble_Bath_2000', 'Flower_Waterer__Activity__4025', 'Chow_Time_3010', 'Mushroom_Sorter__Assessment__4090', '17113b36', 'e37a2b78', 'Bird_Measurer__Assessment__3020', 'Bird_Measurer__Assessment__4110', 'Bird_Measurer__Assessment__3120', 'Mushroom_Sorter__Assessment__4080', 'Dino_Dive_4080', 'Dino_Drink_4010', 'Dino_Drink_4080', 'Egg_Dropper__Activity__4020', 'Leaf_Leader_2000', 'Dino_Drink_4070', '5290eab1', 'Cauldron_Filler__Assessment__3120', 'Cauldron_Filler__Assessment__3020', '6cf7d25c', 'Pan_Balance_3110', 'Pan_Balance_3010', '4b5efe37', 'All_Star_Sorting_4010', 'All_Star_Sorting_2000', 'Chicken_Balancer__Activity__4030', 'Chicken_Balancer__Activity__2000', 'Leaf_Leader_4080', 'Watering_Hole__Activity__4070', 'Air_Show_2060', 'Dino_Dive_2070', '8d7e386c', 'Happy_Camel_3110', 'Happy_Camel_3010', 'Happy_Camel_4035', 'Scrub_A_Dub_2081', 'Bottle_Filler__Activity__2000', 'Bird_Measurer__Assessment__2010', 'Bug_Measurer__Activity__4030', 'Air_Show_2070', 'Sandcastle_Builder__Activity__4070', 'Flower_Waterer__Activity__4030', '7423acbc', 'Air_Show_3020', 'Air_Show_3120', 'Dino_Dive_2020', 'Dino_Dive_4020', 'Chow_Time_2000', 'f71c4741', 'Scrub_A_Dub_3110', 'Scrub_A_Dub_3010', 'Watering_Hole__Activity__2010', 'Pan_Balance_4070', 'Pan_Balance_4080', 'Bird_Measurer__Assessment__4070', 'daac11b0', '1f19558b', 'All_Star_Sorting_3121', 'All_Star_Sorting_2030', 'All_Star_Sorting_3021', 'Chow_Time_4020', 'Cart_Balancer__Assessment__4030', 'Chow_Time_2020', '2fb91ec1', 'Watering_Hole__Activity__4020', 'Watering_Hole__Activity__4025', 'Flower_Waterer__Activity__4080', 'Leaf_Leader_4070', 'Cauldron_Filler__Assessment__4080', 'Happy_Camel_4070', 'Sandcastle_Builder__Activity__4030', 'a5be6304', 'Mushroom_Sorter__Assessment__2010', 'Mushroom_Sorter__Assessment__2030', 'Chow_Time_4030', 'Dino_Dive_4070', '709b1251', 'Dino_Dive_3121', 'Dino_Dive_3021', 'Pan_Balance_4030', 'Air_Show_4110', 'Crystals_Rule_2000', '45d01abe', 'Bird_Measurer__Assessment__3021', 'Bird_Measurer__Assessment__3121', 'Flower_Waterer__Activity__4070', 'Happy_Camel_2081', '37ee8496', 'Cauldron_Filler__Assessment__4030', 'Cauldron_Filler__Assessment__4020', 'Cart_Balancer__Assessment__4035', 'Mushroom_Sorter__Assessment__4030', 'Dino_Drink_4020', 'Pan_Balance_4025', '47026d5f', '56817e2b', 'Chow_Time_3021', 'Chow_Time_2030', 'Chow_Time_3121', '3bfd1a65', 'Mushroom_Sorter__Assessment__2000', 'Mushroom_Sorter__Assessment__2025', '55115cbd', 'Bubble_Bath_3021', 'Bubble_Bath_3121', 'Cart_Balancer__Assessment__4100', 'c51d8688', 'Pan_Balance_3120', 'Pan_Balance_3020', '88d4a5be', 'Mushroom_Sorter__Assessment__3120', 'Mushroom_Sorter__Assessment__3020', 'Bottle_Filler__Activity__4070', '0a08139c', 'Bug_Measurer__Activity__3110', 'Bug_Measurer__Activity__3010', 'Scrub_A_Dub_4020', 'Bug_Measurer__Activity__4025', 'Scrub_A_Dub_2083', 'Crystals_Rule_4070', 'Mushroom_Sorter__Assessment__4035', '3bf1cf26', 'Happy_Camel_3120', 'Happy_Camel_3020', 'Happy_Camel_4080', 'Bird_Measurer__Assessment__4100', 'Happy_Camel_2020', 'Air_Show_4010', 'Chicken_Balancer__Activity__4090', 'Leaf_Leader_2020', 'All_Star_Sorting_4035', 'Scrub_A_Dub_4010', 'All_Star_Sorting_4070', 'Scrub_A_Dub_4090', '44cb4907', 'Crystals_Rule_3120', 'Crystals_Rule_3020', 'Bird_Measurer__Assessment__4040', 'e9c52111', 'Bottle_Filler__Activity__2020', 'Bottle_Filler__Activity__2030', 'Chest_Sorter__Assessment__4090', 'Mushroom_Sorter__Assessment__4020', 'Cauldron_Filler__Assessment__2020', '31973d56', 'Cart_Balancer__Assessment__3120', 'Cart_Balancer__Assessment__3020', 'Bubble_Bath_4095', 'Happy_Camel_4095', 'Egg_Dropper__Activity__4080', 'Chicken_Balancer__Activity__4035', '0330ab6a', 'Chow_Time_3120', 'Chow_Time_3020', 'Bird_Measurer__Assessment__4080', 'Crystals_Rule_4020', 'Bird_Measurer__Assessment__4035', 'Leaf_Leader_2070', 'Cart_Balancer__Assessment__3021', 'Fireworks__Activity__4090', 'Happy_Camel_4030', 'Leaf_Leader_2060', 'Bubble_Bath_4040', 'Cauldron_Filler__Assessment__4100', 'Air_Show_4100', 'Chest_Sorter__Assessment__4020', 'Pan_Balance_4020', 'Air_Show_2075', 'Watering_Hole__Activity__4021', 'Cauldron_Filler__Assessment__4025', 'Air_Show_2000', 'Chow_Time_4070', 'Dino_Dive_2000', 'Pan_Balance_2000', 'Cart_Balancer__Assessment__3010', 'Dino_Drink_4030', 'Mushroom_Sorter__Assessment__4040', 'Chow_Time_4095', 'Bubble_Bath_4010', 'Cart_Balancer__Assessment__4090', 'Chow_Time_4035', 'Dino_Drink_4090', 'a1bbe385', 'Air_Show_3110', 'Air_Show_3010', 'Flower_Waterer__Activity__4020', 'Chow_Time_3110', 'Chest_Sorter__Assessment__4030', 'Bug_Measurer__Activity__4090', 'Flower_Waterer__Activity__2000', '5859dfb6', 'Bubble_Bath_3120', 'Bubble_Bath_3020', 'Bug_Measurer__Activity__4080', 'Happy_Camel_4090', 'Happy_Camel_4020', 'Cart_Balancer__Assessment__4080', 'Air_Show_4090', 'Cauldron_Filler__Assessment__4040', 'Dino_Dive_2030', 'Egg_Dropper__Activity__4090', '33505eae', 'Leaf_Leader_3010', 'Leaf_Leader_3110', 'Dino_Drink_2060', 'Chicken_Balancer__Activity__4022', 'Cauldron_Filler__Assessment__4035', 'Dino_Drink_2075', 'Happy_Camel_2080', 'Sandcastle_Builder__Activity__2000', 'Cauldron_Filler__Assessment__4070', 'Bird_Measurer__Assessment__4025', 'Sandcastle_Builder__Activity__4080', 'Sandcastle_Builder__Activity__4090', 'Dino_Dive_4010', 'All_Star_Sorting_4020', 'Bottle_Filler__Activity_', 'Bottle_Filler__Activity__4030', 'Crystals_Rule_4090', 'Chest_Sorter__Assessment__4035', 'Bird_Measurer__Assessment__4090', 'Bird_Measurer__Assessment__4020', 'Chest_Sorter__Assessment__4100', 'Bottle_Filler__Activity__4035', 'Chest_Sorter__Assessment__4025', 'Dino_Drink_2000', 'Happy_Camel_4040', 'Air_Show_2020', 'Bug_Measurer__Activity__4070', 'Welcome_to_Lost_Lagoon__2000', 'Tree_Top_City___Level_3_2000', 'Costume_Box_2000', 'Heavy__Heavier__Heaviest_2000', 'Magma_Peak___Level_2_2000', 'Ordering_Spheres_2000', 'Crystal_Caves___Level_2_2000', 'Magma_Peak___Level_1_2000', 'Slop_Problem_2000', 'Crystal_Caves___Level_3_2000', 'Balancing_Act_2000', '12_Monkeys_2000', 'Honey_Cake_2000', 'Crystal_Caves___Level_1_2000', 'Treasure_Map_2000', 'Rulers_2000', 'Tree_Top_City___Level_2_2000', 'Tree_Top_City___Level_1_2000', 'Lifting_Heavy_Things_2000', 'Pirate_s_Tale_2000', 'var_title_event_code']\n",
    "features = sorted(features)\n",
    "# counter = 0\n",
    "# # to_remove = []\n",
    "# features = [x for x in features if x not in to_remove]\n",
    "# for feat_a in features:\n",
    "#     for feat_b in features:\n",
    "#         if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n",
    "#             c = np.corrcoef(reduce_train[feat_a], reduce_train[feat_b])[0][1]\n",
    "#             if c > 0.995:\n",
    "#                 counter += 1\n",
    "#                 to_remove.append(feat_b)\n",
    "#                 print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "003cd2ee 0.0 0.0\n",
      "01ca3a3c 0.00040592652729855896 0.0\n",
      "0ce40006 0.000761112238684798 0.0\n",
      "119b5b02 0.00025370407956159933 0.0\n",
      "13f56524 0.03942561396387254 0.0\n",
      "17ca3959 0.0 0.0\n",
      "1b54d27f 0.0006596306068601583 0.0\n",
      "29a42aea 0.0036533387456870307 0.0\n",
      "2ec694de 0.00806778973005886 0.0\n",
      "4074bac2 0.0 0.0\n",
      "5dc079d8 0.0 0.0\n",
      "611485c5 0.0012177795818956768 0.0\n",
      "6aeafed4 0.13415871727217374 0.008 0.0\n",
      "7fd1ac25 0.017759285569311955 0.0\n",
      "a8cc6fec 0.0 0.0\n",
      "ab4ec3a4 0.0008118530545971179 0.0\n",
      "bfc77bd6 0.01151816521209661 0.0\n",
      "dcb1663e 0.0 0.0\n",
      "e4d32835 0.001167038765983357 0.0\n",
      "eb2c19cd 0.1565354170895068 0.008 0.0\n",
      "ecc6157f 0.006545565252689263 0.0\n",
      "Air_Show_4080 0.0020296326364927946 0.0\n",
      "Air_Show_4090 1.2013395575400851 0.018 0.0\n",
      "Air_Show_4100 15.244570732697381 0.773 0.0\n",
      "Air_Show_4110 3.2098132737974425 0.217 0.0\n",
      "All_Star_Sorting_3021 52.278770042622284 1.44 0.0\n",
      "All_Star_Sorting_4010 0.03470671808402679 0.548 0.0\n",
      "All_Star_Sorting_4080 0.024812258981124418 0.002 0.0\n",
      "Bird_Measurer__Assessment__4040 0.0 0.097 0.0\n",
      "Bottle_Filler__Activity__2010 0.0 0.0\n",
      "Bubble_Bath_3020 5.62182869900548 0.527 0.0\n",
      "Bubble_Bath_4080 0.0014207428455449563 0.0\n",
      "Bug_Measurer__Activity__4080 0.0 0.0\n",
      "Cart_Balancer__Assessment__2000 11.70732697381774 0.822 0.0\n",
      "Cart_Balancer__Assessment__2010 6.664653947635478 0.393 0.0\n",
      "Cart_Balancer__Assessment__2020 5.865993505175563 0.563 0.0\n",
      "Cart_Balancer__Assessment__4080 0.0006088897909478384 0.0\n",
      "Chest_Sorter__Assessment__4080 0.0 0.0\n",
      "Crystals_Rule_2010 0.0 0.0\n",
      "Crystals_Rule_3120 11.686624720925513 0.905 0.0\n",
      "Crystals_Rule_4090 0.0014207428455449563 0.028 0.0\n",
      "Dino_Dive_4080 0.0005074081591231987 0.0\n",
      "Dino_Dive_4090 0.17464988837020498 0.016 0.0\n",
      "Dino_Drink_4080 0.00025370407956159933 0.0\n",
      "Egg_Dropper__Activity__4070 29.645778364116094 1.834 0.0\n",
      "Egg_Dropper__Activity__4080 0.06109194235843313 0.0\n",
      "Egg_Dropper__Activity__4090 0.4418510249644814 0.039 0.0\n",
      "Fireworks__Activity__4080 0.0 0.0\n",
      "Fireworks__Activity__4090 0.0 0.069 0.0\n",
      "Flower_Waterer__Activity__4080 5.074081591231987e-05 0.001 0.0\n",
      "Flower_Waterer__Activity__4090 0.0025370407956159936 0.05 0.0\n",
      "Happy_Camel_2020 0.10386645017251878 1.477 0.0\n",
      "Happy_Camel_2080 0.07580677897300589 0.8 0.0\n",
      "Happy_Camel_4080 0.00020296326364927948 0.0\n",
      "Leaf_Leader_2075 0.012177795818956769 0.24 0.0\n",
      "Leaf_Leader_4080 0.0 0.0\n",
      "Leaf_Leader_4090 0.0 0.013 0.0\n",
      "Mushroom_Sorter__Assessment__4080 0.002638522427440633 0.0\n",
      "Pan_Balance_2010 0.0 0.0\n",
      "Pan_Balance_3121 0.08686827684189162 1.488 0.0\n",
      "Pan_Balance_4080 0.0 0.0\n",
      "Sandcastle_Builder__Activity__2010 5.074081591231987e-05 0.0\n",
      "Sandcastle_Builder__Activity__4080 0.0 0.048 0.0\n",
      "Sandcastle_Builder__Activity__4090 0.0032474122183884717 0.097 0.0\n",
      "Scrub_A_Dub_4080 5.074081591231987e-05 0.0\n",
      "Watering_Hole__Activity__2010 0.00010148163182463974 0.0\n"
     ]
    }
   ],
   "source": [
    "to_exclude = [] \n",
    "ajusted_test = reduce_test.copy()\n",
    "for feature in ajusted_test.columns:\n",
    "    if feature not in ['accuracy_group', 'installation_id', 'accuracy_group', 'session_title']:\n",
    "        data = reduce_train[feature]\n",
    "        train_mean = data.mean()\n",
    "        data = ajusted_test[feature] \n",
    "        test_mean = data.mean()\n",
    "        try:\n",
    "            error = stract_hists(feature, adjust=True)\n",
    "            ajust_factor = train_mean / test_mean\n",
    "            if ajust_factor > 10 or ajust_factor < 0.1:# or error > 0.01:\n",
    "                to_exclude.append(feature)\n",
    "                print(feature, train_mean, test_mean, error)\n",
    "            else:\n",
    "                ajusted_test[feature] *= ajust_factor\n",
    "        except:\n",
    "            to_exclude.append(feature)\n",
    "            print(feature, train_mean, test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19708, 367)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [x for x in features if x not in (to_exclude + to_remove)]\n",
    "reduce_train[features].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03316\tvalid_1's rmse: 1.04951\n",
      "[200]\ttraining's rmse: 0.965692\tvalid_1's rmse: 0.998808\n",
      "[300]\ttraining's rmse: 0.933425\tvalid_1's rmse: 0.984218\n",
      "[400]\ttraining's rmse: 0.91131\tvalid_1's rmse: 0.978819\n",
      "[500]\ttraining's rmse: 0.893319\tvalid_1's rmse: 0.976238\n",
      "[600]\ttraining's rmse: 0.877711\tvalid_1's rmse: 0.974994\n",
      "[700]\ttraining's rmse: 0.863686\tvalid_1's rmse: 0.974762\n",
      "[800]\ttraining's rmse: 0.850724\tvalid_1's rmse: 0.974767\n",
      "Early stopping, best iteration is:\n",
      "[723]\ttraining's rmse: 0.860547\tvalid_1's rmse: 0.974449\n",
      "Partial score of fold 0 is: 0.599480245146407\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03312\tvalid_1's rmse: 1.04302\n",
      "[200]\ttraining's rmse: 0.967473\tvalid_1's rmse: 0.993276\n",
      "[300]\ttraining's rmse: 0.936042\tvalid_1's rmse: 0.978802\n",
      "[400]\ttraining's rmse: 0.913966\tvalid_1's rmse: 0.973023\n",
      "[500]\ttraining's rmse: 0.89637\tvalid_1's rmse: 0.970469\n",
      "[600]\ttraining's rmse: 0.880892\tvalid_1's rmse: 0.969077\n",
      "[700]\ttraining's rmse: 0.867049\tvalid_1's rmse: 0.968538\n",
      "[800]\ttraining's rmse: 0.8542\tvalid_1's rmse: 0.967583\n",
      "[900]\ttraining's rmse: 0.842266\tvalid_1's rmse: 0.967195\n",
      "Early stopping, best iteration is:\n",
      "[892]\ttraining's rmse: 0.843173\tvalid_1's rmse: 0.967178\n",
      "Partial score of fold 1 is: 0.6036950837783097\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03184\tvalid_1's rmse: 1.05083\n",
      "[200]\ttraining's rmse: 0.96519\tvalid_1's rmse: 1.00136\n",
      "[300]\ttraining's rmse: 0.932659\tvalid_1's rmse: 0.985944\n",
      "[400]\ttraining's rmse: 0.911002\tvalid_1's rmse: 0.980109\n",
      "[500]\ttraining's rmse: 0.89336\tvalid_1's rmse: 0.977345\n",
      "[600]\ttraining's rmse: 0.877754\tvalid_1's rmse: 0.975215\n",
      "[700]\ttraining's rmse: 0.86392\tvalid_1's rmse: 0.974256\n",
      "[800]\ttraining's rmse: 0.850929\tvalid_1's rmse: 0.973647\n",
      "[900]\ttraining's rmse: 0.838655\tvalid_1's rmse: 0.973306\n",
      "[1000]\ttraining's rmse: 0.826836\tvalid_1's rmse: 0.972828\n",
      "[1100]\ttraining's rmse: 0.81605\tvalid_1's rmse: 0.972817\n",
      "Early stopping, best iteration is:\n",
      "[1013]\ttraining's rmse: 0.825339\tvalid_1's rmse: 0.972715\n",
      "Partial score of fold 2 is: 0.5972425372007286\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.02872\tvalid_1's rmse: 1.06286\n",
      "[200]\ttraining's rmse: 0.96031\tvalid_1's rmse: 1.01982\n",
      "[300]\ttraining's rmse: 0.927047\tvalid_1's rmse: 1.00868\n",
      "[400]\ttraining's rmse: 0.904757\tvalid_1's rmse: 1.00445\n",
      "[500]\ttraining's rmse: 0.88657\tvalid_1's rmse: 1.00236\n",
      "[600]\ttraining's rmse: 0.871139\tvalid_1's rmse: 1.00147\n",
      "[700]\ttraining's rmse: 0.857097\tvalid_1's rmse: 1.00093\n",
      "[800]\ttraining's rmse: 0.844363\tvalid_1's rmse: 1.00081\n",
      "Early stopping, best iteration is:\n",
      "[737]\ttraining's rmse: 0.852287\tvalid_1's rmse: 1.00069\n",
      "Partial score of fold 3 is: 0.5794999176941528\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.03164\tvalid_1's rmse: 1.04801\n",
      "[200]\ttraining's rmse: 0.965515\tvalid_1's rmse: 0.997899\n",
      "[300]\ttraining's rmse: 0.934263\tvalid_1's rmse: 0.98243\n",
      "[400]\ttraining's rmse: 0.912372\tvalid_1's rmse: 0.976233\n",
      "[500]\ttraining's rmse: 0.894459\tvalid_1's rmse: 0.973173\n",
      "[600]\ttraining's rmse: 0.879276\tvalid_1's rmse: 0.971443\n",
      "[700]\ttraining's rmse: 0.865224\tvalid_1's rmse: 0.970733\n",
      "[800]\ttraining's rmse: 0.852541\tvalid_1's rmse: 0.970083\n",
      "[900]\ttraining's rmse: 0.840612\tvalid_1's rmse: 0.97008\n",
      "Early stopping, best iteration is:\n",
      "[825]\ttraining's rmse: 0.849344\tvalid_1's rmse: 0.969955\n",
      "Partial score of fold 4 is: 0.6003939948830301\n",
      "Our oof cohen kappa score is:  0.5966739097147848\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cat_model = Catb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)\n",
    "lgb_model = Lgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.86263\tval-rmse:1.86364\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.01254\tval-rmse:1.1499\n",
      "[200]\ttrain-rmse:0.741384\tval-rmse:1.00779\n",
      "[300]\ttrain-rmse:0.63737\tval-rmse:0.985878\n",
      "[400]\ttrain-rmse:0.587063\tval-rmse:0.98368\n",
      "Stopping. Best iteration:\n",
      "[390]\ttrain-rmse:0.591285\tval-rmse:0.983629\n",
      "\n",
      "Partial score of fold 0 is: 0.5875508984532429\n",
      "[0]\ttrain-rmse:1.86271\tval-rmse:1.86345\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.01363\tval-rmse:1.14321\n",
      "[200]\ttrain-rmse:0.747092\tval-rmse:0.997699\n",
      "[300]\ttrain-rmse:0.647018\tval-rmse:0.973652\n",
      "[400]\ttrain-rmse:0.590931\tval-rmse:0.970155\n",
      "[500]\ttrain-rmse:0.55507\tval-rmse:0.970147\n",
      "Stopping. Best iteration:\n",
      "[454]\ttrain-rmse:0.569269\tval-rmse:0.969708\n",
      "\n",
      "Partial score of fold 1 is: 0.6004691585343398\n",
      "[0]\ttrain-rmse:1.86273\tval-rmse:1.86358\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.01465\tval-rmse:1.14959\n",
      "[200]\ttrain-rmse:0.746686\tval-rmse:1.00706\n",
      "[300]\ttrain-rmse:0.643088\tval-rmse:0.985684\n",
      "[400]\ttrain-rmse:0.590332\tval-rmse:0.982253\n",
      "[500]\ttrain-rmse:0.556545\tval-rmse:0.981478\n",
      "[600]\ttrain-rmse:0.534784\tval-rmse:0.981427\n",
      "[700]\ttrain-rmse:0.511453\tval-rmse:0.981307\n",
      "Stopping. Best iteration:\n",
      "[669]\ttrain-rmse:0.517492\tval-rmse:0.981217\n",
      "\n",
      "Partial score of fold 2 is: 0.5932101236765068\n",
      "[0]\ttrain-rmse:1.86257\tval-rmse:1.8637\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.01282\tval-rmse:1.16029\n",
      "[200]\ttrain-rmse:0.743516\tval-rmse:1.02402\n",
      "[300]\ttrain-rmse:0.641901\tval-rmse:1.00464\n",
      "[400]\ttrain-rmse:0.582519\tval-rmse:1.00268\n",
      "Stopping. Best iteration:\n",
      "[388]\ttrain-rmse:0.588544\tval-rmse:1.00262\n",
      "\n",
      "Partial score of fold 3 is: 0.5719189802686158\n",
      "[0]\ttrain-rmse:1.86265\tval-rmse:1.86401\n",
      "Multiple eval metrics have been passed: 'val-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until val-rmse hasn't improved in 100 rounds.\n",
      "[100]\ttrain-rmse:1.01663\tval-rmse:1.14721\n",
      "[200]\ttrain-rmse:0.746859\tval-rmse:1.0042\n",
      "[300]\ttrain-rmse:0.646473\tval-rmse:0.982214\n",
      "[400]\ttrain-rmse:0.595047\tval-rmse:0.97944\n",
      "[500]\ttrain-rmse:0.560516\tval-rmse:0.979724\n",
      "Stopping. Best iteration:\n",
      "[422]\ttrain-rmse:0.584311\tval-rmse:0.97913\n",
      "\n",
      "Partial score of fold 4 is: 0.5874852606171882\n",
      "Our oof cohen kappa score is:  0.5884473737187257\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_model = Xgb_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:100: DeprecationWarning: The ``active_features_`` attribute was deprecated in version 0.20 and will be removed 0.22.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19708, 372)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               74600     \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_3 (Layer (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 101,801\n",
      "Trainable params: 101,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15764 samples, validate on 3944 samples\n",
      "Epoch 1/100\n",
      "15712/15764 [============================>.] - ETA: 0s - loss: 1.9166\n",
      "Epoch 00001: val_loss improved from inf to 1.09237, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 4s 272us/sample - loss: 1.9161 - val_loss: 1.0924\n",
      "Epoch 2/100\n",
      "15712/15764 [============================>.] - ETA: 0s - loss: 1.4134\n",
      "Epoch 00002: val_loss improved from 1.09237 to 1.05963, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 181us/sample - loss: 1.4123 - val_loss: 1.0596\n",
      "Epoch 3/100\n",
      "15552/15764 [============================>.] - ETA: 0s - loss: 1.2874\n",
      "Epoch 00003: val_loss improved from 1.05963 to 1.05187, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 180us/sample - loss: 1.2869 - val_loss: 1.0519\n",
      "Epoch 4/100\n",
      "15616/15764 [============================>.] - ETA: 0s - loss: 1.2201\n",
      "Epoch 00004: val_loss did not improve from 1.05187\n",
      "15764/15764 [==============================] - 3s 183us/sample - loss: 1.2208 - val_loss: 1.0524\n",
      "Epoch 5/100\n",
      "15584/15764 [============================>.] - ETA: 0s - loss: 1.1813\n",
      "Epoch 00005: val_loss improved from 1.05187 to 1.04575, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 182us/sample - loss: 1.1815 - val_loss: 1.0458\n",
      "Epoch 6/100\n",
      "15488/15764 [============================>.] - ETA: 0s - loss: 1.1491\n",
      "Epoch 00006: val_loss improved from 1.04575 to 1.03382, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 180us/sample - loss: 1.1519 - val_loss: 1.0338\n",
      "Epoch 7/100\n",
      "15680/15764 [============================>.] - ETA: 0s - loss: 1.1245\n",
      "Epoch 00007: val_loss improved from 1.03382 to 1.02459, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 184us/sample - loss: 1.1249 - val_loss: 1.0246\n",
      "Epoch 8/100\n",
      "15552/15764 [============================>.] - ETA: 0s - loss: 1.1189\n",
      "Epoch 00008: val_loss did not improve from 1.02459\n",
      "15764/15764 [==============================] - 3s 180us/sample - loss: 1.1187 - val_loss: 1.0301\n",
      "Epoch 9/100\n",
      "15520/15764 [============================>.] - ETA: 0s - loss: 1.1066\n",
      "Epoch 00009: val_loss improved from 1.02459 to 1.01631, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 178us/sample - loss: 1.1035 - val_loss: 1.0163\n",
      "Epoch 10/100\n",
      "15648/15764 [============================>.] - ETA: 0s - loss: 1.0911\n",
      "Epoch 00010: val_loss improved from 1.01631 to 1.00794, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 178us/sample - loss: 1.0916 - val_loss: 1.0079\n",
      "Epoch 11/100\n",
      "15520/15764 [============================>.] - ETA: 0s - loss: 1.0652\n",
      "Epoch 00011: val_loss did not improve from 1.00794\n",
      "15764/15764 [==============================] - 3s 196us/sample - loss: 1.0682 - val_loss: 1.0088\n",
      "Epoch 12/100\n",
      "15744/15764 [============================>.] - ETA: 0s - loss: 1.0683\n",
      "Epoch 00012: val_loss did not improve from 1.00794\n",
      "15764/15764 [==============================] - 3s 203us/sample - loss: 1.0683 - val_loss: 1.0135\n",
      "Epoch 13/100\n",
      "15552/15764 [============================>.] - ETA: 0s - loss: 1.0564\n",
      "Epoch 00013: val_loss did not improve from 1.00794\n",
      "15764/15764 [==============================] - 3s 202us/sample - loss: 1.0552 - val_loss: 1.0110\n",
      "Epoch 14/100\n",
      "15584/15764 [============================>.] - ETA: 0s - loss: 1.0436\n",
      "Epoch 00014: val_loss improved from 1.00794 to 1.00638, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 198us/sample - loss: 1.0455 - val_loss: 1.0064\n",
      "Epoch 15/100\n",
      "15520/15764 [============================>.] - ETA: 0s - loss: 1.0260\n",
      "Epoch 00015: val_loss improved from 1.00638 to 1.00461, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 179us/sample - loss: 1.0248 - val_loss: 1.0046\n",
      "Epoch 16/100\n",
      "15712/15764 [============================>.] - ETA: 0s - loss: 1.0270\n",
      "Epoch 00016: val_loss did not improve from 1.00461\n",
      "15764/15764 [==============================] - 3s 176us/sample - loss: 1.0278 - val_loss: 1.0274\n",
      "Epoch 17/100\n",
      "15424/15764 [============================>.] - ETA: 0s - loss: 1.0066\n",
      "Epoch 00017: val_loss improved from 1.00461 to 1.00338, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 179us/sample - loss: 1.0044 - val_loss: 1.0034\n",
      "Epoch 18/100\n",
      "15456/15764 [============================>.] - ETA: 0s - loss: 1.0123\n",
      "Epoch 00018: val_loss improved from 1.00338 to 1.00059, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 180us/sample - loss: 1.0093 - val_loss: 1.0006\n",
      "Epoch 19/100\n",
      "15616/15764 [============================>.] - ETA: 0s - loss: 1.0018\n",
      "Epoch 00019: val_loss improved from 1.00059 to 0.99504, saving model to nn_model.w8\n",
      "15764/15764 [==============================] - 3s 195us/sample - loss: 1.0012 - val_loss: 0.9950\n",
      "Epoch 20/100\n",
      "15520/15764 [============================>.] - ETA: 0s - loss: 0.9917\n",
      "Epoch 00020: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 179us/sample - loss: 0.9905 - val_loss: 1.0051\n",
      "Epoch 21/100\n",
      "15648/15764 [============================>.] - ETA: 0s - loss: 0.9790\n",
      "Epoch 00021: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 184us/sample - loss: 0.9791 - val_loss: 0.9988\n",
      "Epoch 22/100\n",
      "15744/15764 [============================>.] - ETA: 0s - loss: 0.9716\n",
      "Epoch 00022: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 180us/sample - loss: 0.9719 - val_loss: 1.0002\n",
      "Epoch 23/100\n",
      "15520/15764 [============================>.] - ETA: 0s - loss: 0.9631\n",
      "Epoch 00023: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 179us/sample - loss: 0.9657 - val_loss: 1.0234\n",
      "Epoch 24/100\n",
      "15456/15764 [============================>.] - ETA: 0s - loss: 0.9600\n",
      "Epoch 00024: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 181us/sample - loss: 0.9603 - val_loss: 0.9962\n",
      "Epoch 25/100\n",
      "15456/15764 [============================>.] - ETA: 0s - loss: 0.9510\n",
      "Epoch 00025: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 182us/sample - loss: 0.9486 - val_loss: 1.0098\n",
      "Epoch 26/100\n",
      "15648/15764 [============================>.] - ETA: 0s - loss: 0.9441\n",
      "Epoch 00026: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 181us/sample - loss: 0.9435 - val_loss: 1.0039\n",
      "Epoch 27/100\n",
      "15744/15764 [============================>.] - ETA: 0s - loss: 0.9491\n",
      "Epoch 00027: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 179us/sample - loss: 0.9487 - val_loss: 1.0044\n",
      "Epoch 28/100\n",
      "15616/15764 [============================>.] - ETA: 0s - loss: 0.9305\n",
      "Epoch 00028: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 182us/sample - loss: 0.9301 - val_loss: 1.0190\n",
      "Epoch 29/100\n",
      "15584/15764 [============================>.] - ETA: 0s - loss: 0.9296\n",
      "Epoch 00029: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 178us/sample - loss: 0.9291 - val_loss: 1.0417\n",
      "Epoch 30/100\n",
      "15616/15764 [============================>.] - ETA: 0s - loss: 0.9245\n",
      "Epoch 00030: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 176us/sample - loss: 0.9260 - val_loss: 1.0112\n",
      "Epoch 31/100\n",
      "15712/15764 [============================>.] - ETA: 0s - loss: 0.9167\n",
      "Epoch 00031: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 178us/sample - loss: 0.9159 - val_loss: 1.0263\n",
      "Epoch 32/100\n",
      "15584/15764 [============================>.] - ETA: 0s - loss: 0.8982\n",
      "Epoch 00032: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 178us/sample - loss: 0.8981 - val_loss: 1.0040\n",
      "Epoch 33/100\n",
      "15552/15764 [============================>.] - ETA: 0s - loss: 0.8955\n",
      "Epoch 00033: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 179us/sample - loss: 0.8946 - val_loss: 1.0414\n",
      "Epoch 34/100\n",
      "15584/15764 [============================>.] - ETA: 0s - loss: 0.8991\n",
      "Epoch 00034: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 179us/sample - loss: 0.8984 - val_loss: 1.0196\n",
      "Epoch 35/100\n",
      "15488/15764 [============================>.] - ETA: 0s - loss: 0.8788\n",
      "Epoch 00035: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 182us/sample - loss: 0.8792 - val_loss: 1.0139\n",
      "Epoch 36/100\n",
      "15712/15764 [============================>.] - ETA: 0s - loss: 0.8824\n",
      "Epoch 00036: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 177us/sample - loss: 0.8827 - val_loss: 1.0402\n",
      "Epoch 37/100\n",
      "15712/15764 [============================>.] - ETA: 0s - loss: 0.8750\n",
      "Epoch 00037: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 177us/sample - loss: 0.8753 - val_loss: 1.0403\n",
      "Epoch 38/100\n",
      "15424/15764 [============================>.] - ETA: 0s - loss: 0.8781\n",
      "Epoch 00038: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 177us/sample - loss: 0.8790 - val_loss: 1.0153\n",
      "Epoch 39/100\n",
      "15456/15764 [============================>.] - ETA: 0s - loss: 0.8622\n",
      "Epoch 00039: val_loss did not improve from 0.99504\n",
      "15764/15764 [==============================] - 3s 180us/sample - loss: 0.8620 - val_loss: 1.0222\n",
      "Partial score of fold 0 is: 0.5861000319635338\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 200)               74600     \n",
      "_________________________________________________________________\n",
      "layer_normalization_4 (Layer (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_7 (Layer (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 101,801\n",
      "Trainable params: 101,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15766 samples, validate on 3942 samples\n",
      "Epoch 1/100\n",
      "15456/15766 [============================>.] - ETA: 0s - loss: 1.9031\n",
      "Epoch 00001: val_loss improved from inf to 1.09011, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 4s 260us/sample - loss: 1.8949 - val_loss: 1.0901\n",
      "Epoch 2/100\n",
      "15552/15766 [============================>.] - ETA: 0s - loss: 1.4166\n",
      "Epoch 00002: val_loss improved from 1.09011 to 1.06419, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 3s 188us/sample - loss: 1.4157 - val_loss: 1.0642\n",
      "Epoch 3/100\n",
      "15552/15766 [============================>.] - ETA: 0s - loss: 1.3151\n",
      "Epoch 00003: val_loss improved from 1.06419 to 1.02150, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 3s 182us/sample - loss: 1.3148 - val_loss: 1.0215\n",
      "Epoch 4/100\n",
      "15712/15766 [============================>.] - ETA: 0s - loss: 1.2300\n",
      "Epoch 00004: val_loss did not improve from 1.02150\n",
      "15766/15766 [==============================] - 3s 179us/sample - loss: 1.2300 - val_loss: 1.0230\n",
      "Epoch 5/100\n",
      "15648/15766 [============================>.] - ETA: 0s - loss: 1.1969\n",
      "Epoch 00005: val_loss improved from 1.02150 to 1.01136, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 3s 181us/sample - loss: 1.1967 - val_loss: 1.0114\n",
      "Epoch 6/100\n",
      "15744/15766 [============================>.] - ETA: 0s - loss: 1.1693\n",
      "Epoch 00006: val_loss improved from 1.01136 to 1.01032, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 3s 182us/sample - loss: 1.1691 - val_loss: 1.0103\n",
      "Epoch 7/100\n",
      "15648/15766 [============================>.] - ETA: 0s - loss: 1.1406\n",
      "Epoch 00007: val_loss improved from 1.01032 to 0.99425, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 3s 182us/sample - loss: 1.1387 - val_loss: 0.9943\n",
      "Epoch 8/100\n",
      "15744/15766 [============================>.] - ETA: 0s - loss: 1.1198\n",
      "Epoch 00008: val_loss did not improve from 0.99425\n",
      "15766/15766 [==============================] - 3s 178us/sample - loss: 1.1199 - val_loss: 0.9962\n",
      "Epoch 9/100\n",
      "15616/15766 [============================>.] - ETA: 0s - loss: 1.0989\n",
      "Epoch 00009: val_loss did not improve from 0.99425\n",
      "15766/15766 [==============================] - 3s 181us/sample - loss: 1.0991 - val_loss: 1.0136\n",
      "Epoch 10/100\n",
      "15616/15766 [============================>.] - ETA: 0s - loss: 1.0899\n",
      "Epoch 00010: val_loss did not improve from 0.99425\n",
      "15766/15766 [==============================] - 3s 179us/sample - loss: 1.0877 - val_loss: 0.9972\n",
      "Epoch 11/100\n",
      "15616/15766 [============================>.] - ETA: 0s - loss: 1.0785\n",
      "Epoch 00011: val_loss improved from 0.99425 to 0.98514, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 3s 179us/sample - loss: 1.0782 - val_loss: 0.9851\n",
      "Epoch 12/100\n",
      "15712/15766 [============================>.] - ETA: 0s - loss: 1.0656\n",
      "Epoch 00012: val_loss improved from 0.98514 to 0.97890, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 3s 179us/sample - loss: 1.0661 - val_loss: 0.9789\n",
      "Epoch 13/100\n",
      "15488/15766 [============================>.] - ETA: 0s - loss: 1.0582\n",
      "Epoch 00013: val_loss did not improve from 0.97890\n",
      "15766/15766 [==============================] - 3s 180us/sample - loss: 1.0569 - val_loss: 0.9796\n",
      "Epoch 14/100\n",
      "15648/15766 [============================>.] - ETA: 0s - loss: 1.0358\n",
      "Epoch 00014: val_loss improved from 0.97890 to 0.97724, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 3s 179us/sample - loss: 1.0363 - val_loss: 0.9772\n",
      "Epoch 15/100\n",
      "15648/15766 [============================>.] - ETA: 0s - loss: 1.0322\n",
      "Epoch 00015: val_loss did not improve from 0.97724\n",
      "15766/15766 [==============================] - 3s 176us/sample - loss: 1.0326 - val_loss: 0.9782\n",
      "Epoch 16/100\n",
      "15712/15766 [============================>.] - ETA: 0s - loss: 1.0263\n",
      "Epoch 00016: val_loss improved from 0.97724 to 0.97649, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 3s 181us/sample - loss: 1.0255 - val_loss: 0.9765\n",
      "Epoch 17/100\n",
      "15520/15766 [============================>.] - ETA: 0s - loss: 1.0219\n",
      "Epoch 00017: val_loss did not improve from 0.97649\n",
      "15766/15766 [==============================] - 3s 177us/sample - loss: 1.0240 - val_loss: 0.9810\n",
      "Epoch 18/100\n",
      "15616/15766 [============================>.] - ETA: 0s - loss: 1.0044\n",
      "Epoch 00018: val_loss did not improve from 0.97649\n",
      "15766/15766 [==============================] - 3s 179us/sample - loss: 1.0040 - val_loss: 0.9877\n",
      "Epoch 19/100\n",
      "15424/15766 [============================>.] - ETA: 0s - loss: 0.9980\n",
      "Epoch 00019: val_loss did not improve from 0.97649\n",
      "15766/15766 [==============================] - 3s 191us/sample - loss: 0.9961 - val_loss: 0.9895\n",
      "Epoch 20/100\n",
      "15552/15766 [============================>.] - ETA: 0s - loss: 0.9932\n",
      "Epoch 00020: val_loss did not improve from 0.97649\n",
      "15766/15766 [==============================] - 3s 180us/sample - loss: 0.9952 - val_loss: 0.9790\n",
      "Epoch 21/100\n",
      "15488/15766 [============================>.] - ETA: 0s - loss: 0.9799\n",
      "Epoch 00021: val_loss did not improve from 0.97649\n",
      "15766/15766 [==============================] - 3s 188us/sample - loss: 0.9798 - val_loss: 0.9919\n",
      "Epoch 22/100\n",
      "15712/15766 [============================>.] - ETA: 0s - loss: 0.9745\n",
      "Epoch 00022: val_loss did not improve from 0.97649\n",
      "15766/15766 [==============================] - 3s 180us/sample - loss: 0.9742 - val_loss: 0.9799\n",
      "Epoch 23/100\n",
      "15456/15766 [============================>.] - ETA: 0s - loss: 0.9613\n",
      "Epoch 00023: val_loss improved from 0.97649 to 0.97104, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 3s 188us/sample - loss: 0.9650 - val_loss: 0.9710\n",
      "Epoch 24/100\n",
      "15552/15766 [============================>.] - ETA: 0s - loss: 0.9686\n",
      "Epoch 00024: val_loss did not improve from 0.97104\n",
      "15766/15766 [==============================] - 3s 183us/sample - loss: 0.9672 - val_loss: 0.9817\n",
      "Epoch 25/100\n",
      "15616/15766 [============================>.] - ETA: 0s - loss: 0.9547\n",
      "Epoch 00025: val_loss improved from 0.97104 to 0.96392, saving model to nn_model.w8\n",
      "15766/15766 [==============================] - 3s 185us/sample - loss: 0.9556 - val_loss: 0.9639\n",
      "Epoch 26/100\n",
      "15520/15766 [============================>.] - ETA: 0s - loss: 0.9422\n",
      "Epoch 00026: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 179us/sample - loss: 0.9431 - val_loss: 0.9736\n",
      "Epoch 27/100\n",
      "15680/15766 [============================>.] - ETA: 0s - loss: 0.9308\n",
      "Epoch 00027: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 180us/sample - loss: 0.9300 - val_loss: 0.9720\n",
      "Epoch 28/100\n",
      "15584/15766 [============================>.] - ETA: 0s - loss: 0.9307\n",
      "Epoch 00028: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 181us/sample - loss: 0.9313 - val_loss: 0.9878\n",
      "Epoch 29/100\n",
      "15712/15766 [============================>.] - ETA: 0s - loss: 0.9153\n",
      "Epoch 00029: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 180us/sample - loss: 0.9158 - val_loss: 0.9700\n",
      "Epoch 30/100\n",
      "15744/15766 [============================>.] - ETA: 0s - loss: 0.9109\n",
      "Epoch 00030: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 184us/sample - loss: 0.9108 - val_loss: 0.9843\n",
      "Epoch 31/100\n",
      "15552/15766 [============================>.] - ETA: 0s - loss: 0.9128\n",
      "Epoch 00031: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 182us/sample - loss: 0.9135 - val_loss: 0.9816\n",
      "Epoch 32/100\n",
      "15648/15766 [============================>.] - ETA: 0s - loss: 0.8974\n",
      "Epoch 00032: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 178us/sample - loss: 0.8992 - val_loss: 0.9735\n",
      "Epoch 33/100\n",
      "15520/15766 [============================>.] - ETA: 0s - loss: 0.8983\n",
      "Epoch 00033: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 179us/sample - loss: 0.8986 - val_loss: 0.9816\n",
      "Epoch 34/100\n",
      "15424/15766 [============================>.] - ETA: 0s - loss: 0.8912\n",
      "Epoch 00034: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 180us/sample - loss: 0.8904 - val_loss: 0.9841\n",
      "Epoch 35/100\n",
      "15424/15766 [============================>.] - ETA: 0s - loss: 0.8852\n",
      "Epoch 00035: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 177us/sample - loss: 0.8841 - val_loss: 0.9951\n",
      "Epoch 36/100\n",
      "15520/15766 [============================>.] - ETA: 0s - loss: 0.8717\n",
      "Epoch 00036: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 178us/sample - loss: 0.8704 - val_loss: 0.9995\n",
      "Epoch 37/100\n",
      "15616/15766 [============================>.] - ETA: 0s - loss: 0.8726\n",
      "Epoch 00037: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 182us/sample - loss: 0.8706 - val_loss: 0.9903\n",
      "Epoch 38/100\n",
      "15584/15766 [============================>.] - ETA: 0s - loss: 0.8614\n",
      "Epoch 00038: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 179us/sample - loss: 0.8610 - val_loss: 0.9927\n",
      "Epoch 39/100\n",
      "15744/15766 [============================>.] - ETA: 0s - loss: 0.8610\n",
      "Epoch 00039: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 177us/sample - loss: 0.8615 - val_loss: 1.0021\n",
      "Epoch 40/100\n",
      "15552/15766 [============================>.] - ETA: 0s - loss: 0.8589\n",
      "Epoch 00040: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 179us/sample - loss: 0.8571 - val_loss: 1.0187\n",
      "Epoch 41/100\n",
      "15712/15766 [============================>.] - ETA: 0s - loss: 0.8506\n",
      "Epoch 00041: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 180us/sample - loss: 0.8503 - val_loss: 1.0005\n",
      "Epoch 42/100\n",
      "15648/15766 [============================>.] - ETA: 0s - loss: 0.8420\n",
      "Epoch 00042: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 185us/sample - loss: 0.8417 - val_loss: 0.9975\n",
      "Epoch 43/100\n",
      "15552/15766 [============================>.] - ETA: 0s - loss: 0.8373\n",
      "Epoch 00043: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 180us/sample - loss: 0.8395 - val_loss: 1.0055\n",
      "Epoch 44/100\n",
      "15488/15766 [============================>.] - ETA: 0s - loss: 0.8371\n",
      "Epoch 00044: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 186us/sample - loss: 0.8397 - val_loss: 0.9968\n",
      "Epoch 45/100\n",
      "15680/15766 [============================>.] - ETA: 0s - loss: 0.8346\n",
      "Epoch 00045: val_loss did not improve from 0.96392\n",
      "15766/15766 [==============================] - 3s 179us/sample - loss: 0.8342 - val_loss: 1.0154\n",
      "Partial score of fold 1 is: 0.5841782360522924\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 200)               74600     \n",
      "_________________________________________________________________\n",
      "layer_normalization_8 (Layer (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_9 (Layer (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_10 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_11 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 101,801\n",
      "Trainable params: 101,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15767 samples, validate on 3941 samples\n",
      "Epoch 1/100\n",
      "15584/15767 [============================>.] - ETA: 0s - loss: 1.8578\n",
      "Epoch 00001: val_loss improved from inf to 1.12785, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 4s 251us/sample - loss: 1.8531 - val_loss: 1.1279\n",
      "Epoch 2/100\n",
      "15456/15767 [============================>.] - ETA: 0s - loss: 1.3662\n",
      "Epoch 00002: val_loss improved from 1.12785 to 1.09227, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 181us/sample - loss: 1.3646 - val_loss: 1.0923\n",
      "Epoch 3/100\n",
      "15680/15767 [============================>.] - ETA: 0s - loss: 1.2809\n",
      "Epoch 00003: val_loss improved from 1.09227 to 1.06292, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 178us/sample - loss: 1.2809 - val_loss: 1.0629\n",
      "Epoch 4/100\n",
      "15712/15767 [============================>.] - ETA: 0s - loss: 1.2042\n",
      "Epoch 00004: val_loss improved from 1.06292 to 1.04940, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 179us/sample - loss: 1.2045 - val_loss: 1.0494\n",
      "Epoch 5/100\n",
      "15744/15767 [============================>.] - ETA: 0s - loss: 1.1661\n",
      "Epoch 00005: val_loss improved from 1.04940 to 1.04330, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 187us/sample - loss: 1.1658 - val_loss: 1.0433\n",
      "Epoch 6/100\n",
      "15712/15767 [============================>.] - ETA: 0s - loss: 1.1439\n",
      "Epoch 00006: val_loss improved from 1.04330 to 1.03882, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 182us/sample - loss: 1.1432 - val_loss: 1.0388\n",
      "Epoch 7/100\n",
      "15424/15767 [============================>.] - ETA: 0s - loss: 1.1196\n",
      "Epoch 00007: val_loss improved from 1.03882 to 1.02646, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 179us/sample - loss: 1.1177 - val_loss: 1.0265\n",
      "Epoch 8/100\n",
      "15584/15767 [============================>.] - ETA: 0s - loss: 1.1122\n",
      "Epoch 00008: val_loss improved from 1.02646 to 1.02580, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 180us/sample - loss: 1.1121 - val_loss: 1.0258\n",
      "Epoch 9/100\n",
      "15648/15767 [============================>.] - ETA: 0s - loss: 1.0885\n",
      "Epoch 00009: val_loss improved from 1.02580 to 1.02500, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 191us/sample - loss: 1.0885 - val_loss: 1.0250\n",
      "Epoch 10/100\n",
      "15680/15767 [============================>.] - ETA: 0s - loss: 1.0857\n",
      "Epoch 00010: val_loss improved from 1.02500 to 1.02104, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 212us/sample - loss: 1.0872 - val_loss: 1.0210\n",
      "Epoch 11/100\n",
      "15488/15767 [============================>.] - ETA: 0s - loss: 1.0727\n",
      "Epoch 00011: val_loss improved from 1.02104 to 0.99965, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 194us/sample - loss: 1.0732 - val_loss: 0.9996\n",
      "Epoch 12/100\n",
      "15648/15767 [============================>.] - ETA: 0s - loss: 1.0627\n",
      "Epoch 00012: val_loss did not improve from 0.99965\n",
      "15767/15767 [==============================] - 3s 179us/sample - loss: 1.0619 - val_loss: 1.0047\n",
      "Epoch 13/100\n",
      "15744/15767 [============================>.] - ETA: 0s - loss: 1.0534\n",
      "Epoch 00013: val_loss did not improve from 0.99965\n",
      "15767/15767 [==============================] - 3s 178us/sample - loss: 1.0530 - val_loss: 1.0052\n",
      "Epoch 14/100\n",
      "15616/15767 [============================>.] - ETA: 0s - loss: 1.0495\n",
      "Epoch 00014: val_loss did not improve from 0.99965\n",
      "15767/15767 [==============================] - 3s 177us/sample - loss: 1.0497 - val_loss: 1.0115\n",
      "Epoch 15/100\n",
      "15552/15767 [============================>.] - ETA: 0s - loss: 1.0298\n",
      "Epoch 00015: val_loss improved from 0.99965 to 0.99195, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 180us/sample - loss: 1.0296 - val_loss: 0.9920\n",
      "Epoch 16/100\n",
      "15488/15767 [============================>.] - ETA: 0s - loss: 1.0202\n",
      "Epoch 00016: val_loss did not improve from 0.99195\n",
      "15767/15767 [==============================] - 3s 183us/sample - loss: 1.0167 - val_loss: 1.0029\n",
      "Epoch 17/100\n",
      "15712/15767 [============================>.] - ETA: 0s - loss: 1.0214\n",
      "Epoch 00017: val_loss did not improve from 0.99195\n",
      "15767/15767 [==============================] - 3s 192us/sample - loss: 1.0216 - val_loss: 1.0030\n",
      "Epoch 18/100\n",
      "15712/15767 [============================>.] - ETA: 0s - loss: 0.9996\n",
      "Epoch 00018: val_loss did not improve from 0.99195\n",
      "15767/15767 [==============================] - 3s 178us/sample - loss: 0.9998 - val_loss: 1.0067\n",
      "Epoch 19/100\n",
      "15712/15767 [============================>.] - ETA: 0s - loss: 0.9966\n",
      "Epoch 00019: val_loss did not improve from 0.99195\n",
      "15767/15767 [==============================] - 3s 189us/sample - loss: 0.9961 - val_loss: 0.9943\n",
      "Epoch 20/100\n",
      "15744/15767 [============================>.] - ETA: 0s - loss: 0.9896\n",
      "Epoch 00020: val_loss did not improve from 0.99195\n",
      "15767/15767 [==============================] - 3s 181us/sample - loss: 0.9897 - val_loss: 0.9986\n",
      "Epoch 21/100\n",
      "15456/15767 [============================>.] - ETA: 0s - loss: 0.9719\n",
      "Epoch 00021: val_loss did not improve from 0.99195\n",
      "15767/15767 [==============================] - 3s 180us/sample - loss: 0.9730 - val_loss: 1.0106\n",
      "Epoch 22/100\n",
      "15584/15767 [============================>.] - ETA: 0s - loss: 0.9714\n",
      "Epoch 00022: val_loss improved from 0.99195 to 0.98616, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 182us/sample - loss: 0.9708 - val_loss: 0.9862\n",
      "Epoch 23/100\n",
      "15520/15767 [============================>.] - ETA: 0s - loss: 0.9595\n",
      "Epoch 00023: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 183us/sample - loss: 0.9599 - val_loss: 0.9918\n",
      "Epoch 24/100\n",
      "15712/15767 [============================>.] - ETA: 0s - loss: 0.9579\n",
      "Epoch 00024: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 180us/sample - loss: 0.9587 - val_loss: 1.0086\n",
      "Epoch 25/100\n",
      "15616/15767 [============================>.] - ETA: 0s - loss: 0.9505\n",
      "Epoch 00025: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 178us/sample - loss: 0.9508 - val_loss: 1.0133\n",
      "Epoch 26/100\n",
      "15424/15767 [============================>.] - ETA: 0s - loss: 0.9383\n",
      "Epoch 00026: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 183us/sample - loss: 0.9381 - val_loss: 0.9968\n",
      "Epoch 27/100\n",
      "15616/15767 [============================>.] - ETA: 0s - loss: 0.9378\n",
      "Epoch 00027: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 178us/sample - loss: 0.9361 - val_loss: 0.9922\n",
      "Epoch 28/100\n",
      "15744/15767 [============================>.] - ETA: 0s - loss: 0.9333\n",
      "Epoch 00028: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 176us/sample - loss: 0.9334 - val_loss: 0.9925\n",
      "Epoch 29/100\n",
      "15584/15767 [============================>.] - ETA: 0s - loss: 0.9051\n",
      "Epoch 00029: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 177us/sample - loss: 0.9077 - val_loss: 1.0035\n",
      "Epoch 30/100\n",
      "15680/15767 [============================>.] - ETA: 0s - loss: 0.9062\n",
      "Epoch 00030: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 179us/sample - loss: 0.9057 - val_loss: 1.0181\n",
      "Epoch 31/100\n",
      "15648/15767 [============================>.] - ETA: 0s - loss: 0.9073\n",
      "Epoch 00031: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 178us/sample - loss: 0.9065 - val_loss: 1.0006\n",
      "Epoch 32/100\n",
      "15680/15767 [============================>.] - ETA: 0s - loss: 0.8920\n",
      "Epoch 00032: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 179us/sample - loss: 0.8938 - val_loss: 0.9908\n",
      "Epoch 33/100\n",
      "15552/15767 [============================>.] - ETA: 0s - loss: 0.8787\n",
      "Epoch 00033: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 182us/sample - loss: 0.8801 - val_loss: 0.9961\n",
      "Epoch 34/100\n",
      "15552/15767 [============================>.] - ETA: 0s - loss: 0.8902\n",
      "Epoch 00034: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 202us/sample - loss: 0.8911 - val_loss: 0.9908\n",
      "Epoch 35/100\n",
      "15520/15767 [============================>.] - ETA: 0s - loss: 0.8700\n",
      "Epoch 00035: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 207us/sample - loss: 0.8702 - val_loss: 1.0198\n",
      "Epoch 36/100\n",
      "15648/15767 [============================>.] - ETA: 0s - loss: 0.8633\n",
      "Epoch 00036: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 205us/sample - loss: 0.8635 - val_loss: 1.0157\n",
      "Epoch 37/100\n",
      "15680/15767 [============================>.] - ETA: 0s - loss: 0.8700\n",
      "Epoch 00037: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 201us/sample - loss: 0.8694 - val_loss: 1.0191\n",
      "Epoch 38/100\n",
      "15488/15767 [============================>.] - ETA: 0s - loss: 0.8672\n",
      "Epoch 00038: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 190us/sample - loss: 0.8635 - val_loss: 1.0133\n",
      "Epoch 39/100\n",
      "15648/15767 [============================>.] - ETA: 0s - loss: 0.8511\n",
      "Epoch 00039: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 187us/sample - loss: 0.8515 - val_loss: 1.0167\n",
      "Epoch 40/100\n",
      "15584/15767 [============================>.] - ETA: 0s - loss: 0.8487\n",
      "Epoch 00040: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 185us/sample - loss: 0.8485 - val_loss: 1.0334\n",
      "Epoch 41/100\n",
      "15488/15767 [============================>.] - ETA: 0s - loss: 0.8444\n",
      "Epoch 00041: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 179us/sample - loss: 0.8439 - val_loss: 1.0211\n",
      "Epoch 42/100\n",
      "15584/15767 [============================>.] - ETA: 0s - loss: 0.8393\n",
      "Epoch 00042: val_loss did not improve from 0.98616\n",
      "15767/15767 [==============================] - 3s 179us/sample - loss: 0.8399 - val_loss: 1.0058\n",
      "Partial score of fold 2 is: 0.5835323312183746\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 200)               74600     \n",
      "_________________________________________________________________\n",
      "layer_normalization_12 (Laye (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_13 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_14 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_15 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 101,801\n",
      "Trainable params: 101,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15767 samples, validate on 3941 samples\n",
      "Epoch 1/100\n",
      "15712/15767 [============================>.] - ETA: 0s - loss: 1.7088\n",
      "Epoch 00001: val_loss improved from inf to 1.15771, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 4s 254us/sample - loss: 1.7071 - val_loss: 1.1577\n",
      "Epoch 2/100\n",
      "15648/15767 [============================>.] - ETA: 0s - loss: 1.3135\n",
      "Epoch 00002: val_loss improved from 1.15771 to 1.10012, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 179us/sample - loss: 1.3122 - val_loss: 1.1001\n",
      "Epoch 3/100\n",
      "15712/15767 [============================>.] - ETA: 0s - loss: 1.2234\n",
      "Epoch 00003: val_loss improved from 1.10012 to 1.07662, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 180us/sample - loss: 1.2227 - val_loss: 1.0766\n",
      "Epoch 4/100\n",
      "15520/15767 [============================>.] - ETA: 0s - loss: 1.1555\n",
      "Epoch 00004: val_loss improved from 1.07662 to 1.06727, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 184us/sample - loss: 1.1523 - val_loss: 1.0673\n",
      "Epoch 5/100\n",
      "15712/15767 [============================>.] - ETA: 0s - loss: 1.1420\n",
      "Epoch 00005: val_loss improved from 1.06727 to 1.06561, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 181us/sample - loss: 1.1420 - val_loss: 1.0656\n",
      "Epoch 6/100\n",
      "15648/15767 [============================>.] - ETA: 0s - loss: 1.1197\n",
      "Epoch 00006: val_loss did not improve from 1.06561\n",
      "15767/15767 [==============================] - 3s 180us/sample - loss: 1.1194 - val_loss: 1.0722\n",
      "Epoch 7/100\n",
      "15456/15767 [============================>.] - ETA: 0s - loss: 1.1028\n",
      "Epoch 00007: val_loss improved from 1.06561 to 1.05987, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 183us/sample - loss: 1.1008 - val_loss: 1.0599\n",
      "Epoch 8/100\n",
      "15680/15767 [============================>.] - ETA: 0s - loss: 1.0766\n",
      "Epoch 00008: val_loss did not improve from 1.05987\n",
      "15767/15767 [==============================] - 3s 178us/sample - loss: 1.0784 - val_loss: 1.0603\n",
      "Epoch 9/100\n",
      "15552/15767 [============================>.] - ETA: 0s - loss: 1.0626\n",
      "Epoch 00009: val_loss improved from 1.05987 to 1.05784, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 178us/sample - loss: 1.0640 - val_loss: 1.0578\n",
      "Epoch 10/100\n",
      "15616/15767 [============================>.] - ETA: 0s - loss: 1.0598\n",
      "Epoch 00010: val_loss did not improve from 1.05784\n",
      "15767/15767 [==============================] - 3s 176us/sample - loss: 1.0580 - val_loss: 1.0601\n",
      "Epoch 11/100\n",
      "15680/15767 [============================>.] - ETA: 0s - loss: 1.0551\n",
      "Epoch 00011: val_loss improved from 1.05784 to 1.05368, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 182us/sample - loss: 1.0541 - val_loss: 1.0537\n",
      "Epoch 12/100\n",
      "15520/15767 [============================>.] - ETA: 0s - loss: 1.0457\n",
      "Epoch 00012: val_loss did not improve from 1.05368\n",
      "15767/15767 [==============================] - 3s 176us/sample - loss: 1.0450 - val_loss: 1.0613\n",
      "Epoch 13/100\n",
      "15680/15767 [============================>.] - ETA: 0s - loss: 1.0284\n",
      "Epoch 00013: val_loss did not improve from 1.05368\n",
      "15767/15767 [==============================] - 3s 177us/sample - loss: 1.0284 - val_loss: 1.0614\n",
      "Epoch 14/100\n",
      "15648/15767 [============================>.] - ETA: 0s - loss: 1.0256\n",
      "Epoch 00014: val_loss improved from 1.05368 to 1.04775, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 217us/sample - loss: 1.0275 - val_loss: 1.0478\n",
      "Epoch 15/100\n",
      "15584/15767 [============================>.] - ETA: 0s - loss: 1.0202\n",
      "Epoch 00015: val_loss improved from 1.04775 to 1.04623, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 181us/sample - loss: 1.0219 - val_loss: 1.0462\n",
      "Epoch 16/100\n",
      "15456/15767 [============================>.] - ETA: 0s - loss: 1.0064\n",
      "Epoch 00016: val_loss improved from 1.04623 to 1.04024, saving model to nn_model.w8\n",
      "15767/15767 [==============================] - 3s 196us/sample - loss: 1.0089 - val_loss: 1.0402\n",
      "Epoch 17/100\n",
      "15616/15767 [============================>.] - ETA: 0s - loss: 0.9962\n",
      "Epoch 00017: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 186us/sample - loss: 0.9980 - val_loss: 1.0504\n",
      "Epoch 18/100\n",
      "15616/15767 [============================>.] - ETA: 0s - loss: 0.9883\n",
      "Epoch 00018: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 185us/sample - loss: 0.9872 - val_loss: 1.0477\n",
      "Epoch 19/100\n",
      "15712/15767 [============================>.] - ETA: 0s - loss: 0.9875\n",
      "Epoch 00019: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 181us/sample - loss: 0.9886 - val_loss: 1.0410\n",
      "Epoch 20/100\n",
      "15744/15767 [============================>.] - ETA: 0s - loss: 0.9762\n",
      "Epoch 00020: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 177us/sample - loss: 0.9758 - val_loss: 1.0440\n",
      "Epoch 21/100\n",
      "15584/15767 [============================>.] - ETA: 0s - loss: 0.9681\n",
      "Epoch 00021: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 179us/sample - loss: 0.9692 - val_loss: 1.0499\n",
      "Epoch 22/100\n",
      "15520/15767 [============================>.] - ETA: 0s - loss: 0.9605\n",
      "Epoch 00022: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 179us/sample - loss: 0.9606 - val_loss: 1.0527\n",
      "Epoch 23/100\n",
      "15744/15767 [============================>.] - ETA: 0s - loss: 0.9542\n",
      "Epoch 00023: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 178us/sample - loss: 0.9541 - val_loss: 1.0489\n",
      "Epoch 24/100\n",
      "15520/15767 [============================>.] - ETA: 0s - loss: 0.9390\n",
      "Epoch 00024: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 177us/sample - loss: 0.9404 - val_loss: 1.0456\n",
      "Epoch 25/100\n",
      "15680/15767 [============================>.] - ETA: 0s - loss: 0.9331\n",
      "Epoch 00025: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 184us/sample - loss: 0.9323 - val_loss: 1.0574\n",
      "Epoch 26/100\n",
      "15616/15767 [============================>.] - ETA: 0s - loss: 0.9241\n",
      "Epoch 00026: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 179us/sample - loss: 0.9246 - val_loss: 1.0414\n",
      "Epoch 27/100\n",
      "15744/15767 [============================>.] - ETA: 0s - loss: 0.9294\n",
      "Epoch 00027: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 177us/sample - loss: 0.9291 - val_loss: 1.0425\n",
      "Epoch 28/100\n",
      "15488/15767 [============================>.] - ETA: 0s - loss: 0.9250\n",
      "Epoch 00028: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 185us/sample - loss: 0.9248 - val_loss: 1.0502\n",
      "Epoch 29/100\n",
      "15584/15767 [============================>.] - ETA: 0s - loss: 0.9153\n",
      "Epoch 00029: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 178us/sample - loss: 0.9146 - val_loss: 1.0417\n",
      "Epoch 30/100\n",
      "15744/15767 [============================>.] - ETA: 0s - loss: 0.9033\n",
      "Epoch 00030: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 177us/sample - loss: 0.9033 - val_loss: 1.0725\n",
      "Epoch 31/100\n",
      "15680/15767 [============================>.] - ETA: 0s - loss: 0.9015\n",
      "Epoch 00031: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 177us/sample - loss: 0.9026 - val_loss: 1.0480\n",
      "Epoch 32/100\n",
      "15744/15767 [============================>.] - ETA: 0s - loss: 0.8903\n",
      "Epoch 00032: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 181us/sample - loss: 0.8904 - val_loss: 1.0483\n",
      "Epoch 33/100\n",
      "15488/15767 [============================>.] - ETA: 0s - loss: 0.8859\n",
      "Epoch 00033: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 177us/sample - loss: 0.8868 - val_loss: 1.0710\n",
      "Epoch 34/100\n",
      "15456/15767 [============================>.] - ETA: 0s - loss: 0.8874\n",
      "Epoch 00034: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 176us/sample - loss: 0.8869 - val_loss: 1.0528\n",
      "Epoch 35/100\n",
      "15456/15767 [============================>.] - ETA: 0s - loss: 0.8780\n",
      "Epoch 00035: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 180us/sample - loss: 0.8792 - val_loss: 1.0505\n",
      "Epoch 36/100\n",
      "15520/15767 [============================>.] - ETA: 0s - loss: 0.8713\n",
      "Epoch 00036: val_loss did not improve from 1.04024\n",
      "15767/15767 [==============================] - 3s 176us/sample - loss: 0.8720 - val_loss: 1.0511\n",
      "Partial score of fold 3 is: 0.5598217396959505\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 200)               74600     \n",
      "_________________________________________________________________\n",
      "layer_normalization_16 (Laye (None, 200)               400       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "layer_normalization_17 (Laye (None, 100)               200       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "layer_normalization_18 (Laye (None, 50)                100       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "layer_normalization_19 (Laye (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 101,801\n",
      "Trainable params: 101,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15768 samples, validate on 3940 samples\n",
      "Epoch 1/100\n",
      "15744/15768 [============================>.] - ETA: 0s - loss: 2.0519\n",
      "Epoch 00001: val_loss improved from inf to 1.18690, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 4s 254us/sample - loss: 2.0519 - val_loss: 1.1869\n",
      "Epoch 2/100\n",
      "15648/15768 [============================>.] - ETA: 0s - loss: 1.4130\n",
      "Epoch 00002: val_loss improved from 1.18690 to 1.11094, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 190us/sample - loss: 1.4112 - val_loss: 1.1109\n",
      "Epoch 3/100\n",
      "15456/15768 [============================>.] - ETA: 0s - loss: 1.3022\n",
      "Epoch 00003: val_loss improved from 1.11094 to 1.07553, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 181us/sample - loss: 1.2988 - val_loss: 1.0755\n",
      "Epoch 4/100\n",
      "15648/15768 [============================>.] - ETA: 0s - loss: 1.2179\n",
      "Epoch 00004: val_loss improved from 1.07553 to 1.04204, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 180us/sample - loss: 1.2192 - val_loss: 1.0420\n",
      "Epoch 5/100\n",
      "15520/15768 [============================>.] - ETA: 0s - loss: 1.1892\n",
      "Epoch 00005: val_loss improved from 1.04204 to 1.03930, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 178us/sample - loss: 1.1890 - val_loss: 1.0393\n",
      "Epoch 6/100\n",
      "15456/15768 [============================>.] - ETA: 0s - loss: 1.1587\n",
      "Epoch 00006: val_loss improved from 1.03930 to 1.03015, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 182us/sample - loss: 1.1589 - val_loss: 1.0301\n",
      "Epoch 7/100\n",
      "15744/15768 [============================>.] - ETA: 0s - loss: 1.1386\n",
      "Epoch 00007: val_loss did not improve from 1.03015\n",
      "15768/15768 [==============================] - 3s 178us/sample - loss: 1.1389 - val_loss: 1.0379\n",
      "Epoch 8/100\n",
      "15584/15768 [============================>.] - ETA: 0s - loss: 1.1245\n",
      "Epoch 00008: val_loss did not improve from 1.03015\n",
      "15768/15768 [==============================] - 3s 179us/sample - loss: 1.1246 - val_loss: 1.0311\n",
      "Epoch 9/100\n",
      "15552/15768 [============================>.] - ETA: 0s - loss: 1.0986\n",
      "Epoch 00009: val_loss improved from 1.03015 to 1.00836, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 184us/sample - loss: 1.0983 - val_loss: 1.0084\n",
      "Epoch 10/100\n",
      "15456/15768 [============================>.] - ETA: 0s - loss: 1.0946\n",
      "Epoch 00010: val_loss improved from 1.00836 to 1.00732, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 184us/sample - loss: 1.0945 - val_loss: 1.0073\n",
      "Epoch 11/100\n",
      "15712/15768 [============================>.] - ETA: 0s - loss: 1.0838\n",
      "Epoch 00011: val_loss improved from 1.00732 to 1.00390, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 181us/sample - loss: 1.0852 - val_loss: 1.0039\n",
      "Epoch 12/100\n",
      "15648/15768 [============================>.] - ETA: 0s - loss: 1.0765\n",
      "Epoch 00012: val_loss did not improve from 1.00390\n",
      "15768/15768 [==============================] - 3s 178us/sample - loss: 1.0763 - val_loss: 1.0075\n",
      "Epoch 13/100\n",
      "15520/15768 [============================>.] - ETA: 0s - loss: 1.0581\n",
      "Epoch 00013: val_loss improved from 1.00390 to 0.99435, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 183us/sample - loss: 1.0563 - val_loss: 0.9944\n",
      "Epoch 14/100\n",
      "15520/15768 [============================>.] - ETA: 0s - loss: 1.0504\n",
      "Epoch 00014: val_loss improved from 0.99435 to 0.98586, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 178us/sample - loss: 1.0523 - val_loss: 0.9859\n",
      "Epoch 15/100\n",
      "15488/15768 [============================>.] - ETA: 0s - loss: 1.0443\n",
      "Epoch 00015: val_loss did not improve from 0.98586\n",
      "15768/15768 [==============================] - 3s 181us/sample - loss: 1.0458 - val_loss: 0.9859\n",
      "Epoch 16/100\n",
      "15456/15768 [============================>.] - ETA: 0s - loss: 1.0348\n",
      "Epoch 00016: val_loss improved from 0.98586 to 0.98253, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 184us/sample - loss: 1.0326 - val_loss: 0.9825\n",
      "Epoch 17/100\n",
      "15520/15768 [============================>.] - ETA: 0s - loss: 1.0255\n",
      "Epoch 00017: val_loss improved from 0.98253 to 0.97814, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 180us/sample - loss: 1.0240 - val_loss: 0.9781\n",
      "Epoch 18/100\n",
      "15616/15768 [============================>.] - ETA: 0s - loss: 1.0135\n",
      "Epoch 00018: val_loss improved from 0.97814 to 0.96925, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 187us/sample - loss: 1.0127 - val_loss: 0.9693\n",
      "Epoch 19/100\n",
      "15648/15768 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 00019: val_loss did not improve from 0.96925\n",
      "15768/15768 [==============================] - 3s 179us/sample - loss: 1.0004 - val_loss: 0.9828\n",
      "Epoch 20/100\n",
      "15488/15768 [============================>.] - ETA: 0s - loss: 1.0019\n",
      "Epoch 00020: val_loss did not improve from 0.96925\n",
      "15768/15768 [==============================] - 3s 180us/sample - loss: 1.0021 - val_loss: 0.9846\n",
      "Epoch 21/100\n",
      "15488/15768 [============================>.] - ETA: 0s - loss: 0.9919\n",
      "Epoch 00021: val_loss did not improve from 0.96925\n",
      "15768/15768 [==============================] - 3s 192us/sample - loss: 0.9933 - val_loss: 0.9838\n",
      "Epoch 22/100\n",
      "15552/15768 [============================>.] - ETA: 0s - loss: 0.9830\n",
      "Epoch 00022: val_loss did not improve from 0.96925\n",
      "15768/15768 [==============================] - 3s 180us/sample - loss: 0.9851 - val_loss: 0.9704\n",
      "Epoch 23/100\n",
      "15744/15768 [============================>.] - ETA: 0s - loss: 0.9647\n",
      "Epoch 00023: val_loss did not improve from 0.96925\n",
      "15768/15768 [==============================] - 3s 183us/sample - loss: 0.9645 - val_loss: 0.9772\n",
      "Epoch 24/100\n",
      "15648/15768 [============================>.] - ETA: 0s - loss: 0.9589\n",
      "Epoch 00024: val_loss did not improve from 0.96925\n",
      "15768/15768 [==============================] - 3s 179us/sample - loss: 0.9592 - val_loss: 0.9810\n",
      "Epoch 25/100\n",
      "15456/15768 [============================>.] - ETA: 0s - loss: 0.9600\n",
      "Epoch 00025: val_loss did not improve from 0.96925\n",
      "15768/15768 [==============================] - 3s 177us/sample - loss: 0.9594 - val_loss: 0.9912\n",
      "Epoch 26/100\n",
      "15648/15768 [============================>.] - ETA: 0s - loss: 0.9544\n",
      "Epoch 00026: val_loss did not improve from 0.96925\n",
      "15768/15768 [==============================] - 3s 181us/sample - loss: 0.9554 - val_loss: 0.9735\n",
      "Epoch 27/100\n",
      "15520/15768 [============================>.] - ETA: 0s - loss: 0.9366\n",
      "Epoch 00027: val_loss improved from 0.96925 to 0.96469, saving model to nn_model.w8\n",
      "15768/15768 [==============================] - 3s 184us/sample - loss: 0.9385 - val_loss: 0.9647\n",
      "Epoch 28/100\n",
      "15712/15768 [============================>.] - ETA: 0s - loss: 0.9375\n",
      "Epoch 00028: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 184us/sample - loss: 0.9371 - val_loss: 0.9883\n",
      "Epoch 29/100\n",
      "15680/15768 [============================>.] - ETA: 0s - loss: 0.9368\n",
      "Epoch 00029: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 180us/sample - loss: 0.9362 - val_loss: 0.9783\n",
      "Epoch 30/100\n",
      "15648/15768 [============================>.] - ETA: 0s - loss: 0.9284\n",
      "Epoch 00030: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 181us/sample - loss: 0.9291 - val_loss: 0.9666\n",
      "Epoch 31/100\n",
      "15648/15768 [============================>.] - ETA: 0s - loss: 0.9147\n",
      "Epoch 00031: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 182us/sample - loss: 0.9156 - val_loss: 0.9734\n",
      "Epoch 32/100\n",
      "15488/15768 [============================>.] - ETA: 0s - loss: 0.9121\n",
      "Epoch 00032: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 180us/sample - loss: 0.9123 - val_loss: 0.9755\n",
      "Epoch 33/100\n",
      "15584/15768 [============================>.] - ETA: 0s - loss: 0.9049\n",
      "Epoch 00033: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 178us/sample - loss: 0.9041 - val_loss: 0.9797\n",
      "Epoch 34/100\n",
      "15456/15768 [============================>.] - ETA: 0s - loss: 0.8938\n",
      "Epoch 00034: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 183us/sample - loss: 0.8944 - val_loss: 0.9929\n",
      "Epoch 35/100\n",
      "15616/15768 [============================>.] - ETA: 0s - loss: 0.8848\n",
      "Epoch 00035: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 181us/sample - loss: 0.8864 - val_loss: 1.0092\n",
      "Epoch 36/100\n",
      "15616/15768 [============================>.] - ETA: 0s - loss: 0.8876\n",
      "Epoch 00036: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 191us/sample - loss: 0.8891 - val_loss: 0.9817\n",
      "Epoch 37/100\n",
      "15616/15768 [============================>.] - ETA: 0s - loss: 0.8872\n",
      "Epoch 00037: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 4s 223us/sample - loss: 0.8883 - val_loss: 0.9876\n",
      "Epoch 38/100\n",
      "15520/15768 [============================>.] - ETA: 0s - loss: 0.8800\n",
      "Epoch 00038: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 199us/sample - loss: 0.8778 - val_loss: 0.9816\n",
      "Epoch 39/100\n",
      "15456/15768 [============================>.] - ETA: 0s - loss: 0.8772\n",
      "Epoch 00039: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 180us/sample - loss: 0.8779 - val_loss: 0.9925\n",
      "Epoch 40/100\n",
      "15488/15768 [============================>.] - ETA: 0s - loss: 0.8677\n",
      "Epoch 00040: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 186us/sample - loss: 0.8677 - val_loss: 0.9773\n",
      "Epoch 41/100\n",
      "15616/15768 [============================>.] - ETA: 0s - loss: 0.8617\n",
      "Epoch 00041: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 190us/sample - loss: 0.8635 - val_loss: 0.9884\n",
      "Epoch 42/100\n",
      "15712/15768 [============================>.] - ETA: 0s - loss: 0.8667\n",
      "Epoch 00042: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 188us/sample - loss: 0.8654 - val_loss: 0.9954\n",
      "Epoch 43/100\n",
      "15488/15768 [============================>.] - ETA: 0s - loss: 0.8536\n",
      "Epoch 00043: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 183us/sample - loss: 0.8541 - val_loss: 0.9961\n",
      "Epoch 44/100\n",
      "15456/15768 [============================>.] - ETA: 0s - loss: 0.8366\n",
      "Epoch 00044: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 180us/sample - loss: 0.8394 - val_loss: 0.9929\n",
      "Epoch 45/100\n",
      "15584/15768 [============================>.] - ETA: 0s - loss: 0.8406\n",
      "Epoch 00045: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 182us/sample - loss: 0.8412 - val_loss: 1.0099\n",
      "Epoch 46/100\n",
      "15520/15768 [============================>.] - ETA: 0s - loss: 0.8324\n",
      "Epoch 00046: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 175us/sample - loss: 0.8309 - val_loss: 1.0018\n",
      "Epoch 47/100\n",
      "15744/15768 [============================>.] - ETA: 0s - loss: 0.8305\n",
      "Epoch 00047: val_loss did not improve from 0.96469\n",
      "15768/15768 [==============================] - 3s 181us/sample - loss: 0.8298 - val_loss: 1.0244\n",
      "Partial score of fold 4 is: 0.5986190439214769\n",
      "Our oof cohen kappa score is:  0.5825758852823228\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_model = Nn_Model(reduce_train, ajusted_test, features, categoricals=categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clparams   = {'n_estimators':2000,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.04,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1, \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100, \n",
    "            'bagging_fraction_seed': 127,\n",
    "            'feature_fraction_seed': 127,\n",
    "            'data_random_seed': 127,\n",
    "            'seed':127\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Wed Jan 22 19:53:03 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:148: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['session_title']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.911608\tvalid_1's auc: 0.860359\n",
      "[200]\ttraining's auc: 0.94207\tvalid_1's auc: 0.865487\n",
      "[300]\ttraining's auc: 0.960668\tvalid_1's auc: 0.867372\n",
      "[400]\ttraining's auc: 0.972932\tvalid_1's auc: 0.867706\n",
      "Early stopping, best iteration is:\n",
      "[363]\ttraining's auc: 0.969046\tvalid_1's auc: 0.868143\n",
      "Fold 2 started at Wed Jan 22 19:53:24 2020\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.910398\tvalid_1's auc: 0.873022\n",
      "[200]\ttraining's auc: 0.942022\tvalid_1's auc: 0.877035\n",
      "[300]\ttraining's auc: 0.960832\tvalid_1's auc: 0.877528\n",
      "Early stopping, best iteration is:\n",
      "[244]\ttraining's auc: 0.951313\tvalid_1's auc: 0.877913\n",
      "Fold 3 started at Wed Jan 22 19:53:43 2020\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.910851\tvalid_1's auc: 0.86844\n",
      "[200]\ttraining's auc: 0.941864\tvalid_1's auc: 0.873152\n",
      "[300]\ttraining's auc: 0.961493\tvalid_1's auc: 0.873673\n",
      "Early stopping, best iteration is:\n",
      "[266]\ttraining's auc: 0.955679\tvalid_1's auc: 0.874343\n",
      "Fold 4 started at Wed Jan 22 19:54:00 2020\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.914371\tvalid_1's auc: 0.85207\n",
      "[200]\ttraining's auc: 0.945113\tvalid_1's auc: 0.854199\n",
      "[300]\ttraining's auc: 0.963181\tvalid_1's auc: 0.854749\n",
      "[400]\ttraining's auc: 0.975126\tvalid_1's auc: 0.855891\n",
      "[500]\ttraining's auc: 0.982671\tvalid_1's auc: 0.855382\n",
      "Early stopping, best iteration is:\n",
      "[405]\ttraining's auc: 0.975529\tvalid_1's auc: 0.856075\n",
      "Fold 5 started at Wed Jan 22 19:54:22 2020\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's auc: 0.911314\tvalid_1's auc: 0.866543\n",
      "[200]\ttraining's auc: 0.942561\tvalid_1's auc: 0.869207\n",
      "[300]\ttraining's auc: 0.961228\tvalid_1's auc: 0.869881\n",
      "Early stopping, best iteration is:\n",
      "[268]\ttraining's auc: 0.955863\tvalid_1's auc: 0.87004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n",
    "n_fold = 5\n",
    "# folds = GroupKFold(n_splits=n_fold)\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "X = reduce_train.copy()\n",
    "cl_y = reduce_train['accuracy_group'].copy()\n",
    "cl_y.loc[cl_y>0]=1\n",
    "cols_to_drop = ['installation_id','accuracy_group']\n",
    "cl_oof = np.zeros(len(reduce_train))\n",
    "models = []\n",
    "for fold_n, (train_index, valid_index) in enumerate(folds.split(X, cl_y, X['installation_id'])):\n",
    "    print('Fold {} started at {}'.format(fold_n+1,time.ctime()))\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "    y_train, y_valid = cl_y.iloc[train_index], cl_y.iloc[valid_index]\n",
    "    \n",
    "    X_train = X_train.drop(cols_to_drop,axis=1)\n",
    "    X_valid = X_valid.drop(cols_to_drop,axis=1)\n",
    "    \n",
    "    trn_data = lgb.Dataset(X_train,label=y_train)\n",
    "    val_data = lgb.Dataset(X_valid,label=y_valid)\n",
    "    \n",
    "    cl_lgb_model = lgb.train(clparams,\n",
    "                        trn_data,\n",
    "                        valid_sets=[trn_data,val_data],\n",
    "                        verbose_eval=100,\n",
    "                        categorical_feature = categoricals\n",
    "                        )\n",
    "    pred = cl_lgb_model.predict(X_valid)\n",
    "    models.append(cl_lgb_model)\n",
    "    cl_oof[valid_index] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5786400523744044\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, loss_score, _ = eval_qwk_lgb_regr(reduce_train['accuracy_group'], cl_oof*3)\n",
    "print (loss_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cl_predict(x,models):\n",
    "    all_ans = np.zeros((len(x)))\n",
    "    cols_to_drop = ['installation_id','accuracy_group']\n",
    "    test_copy = x.drop(cols_to_drop,axis=1)\n",
    "    for model in models:\n",
    "        ans = model.predict(test_copy)\n",
    "        all_ans += ans\n",
    "        \n",
    "    return all_ans/n_fold\n",
    "\n",
    "class_pred = cl_predict(ajusted_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "weights = {'lbg': 0.4, 'cls': 0.2, 'xgb': 0.3, 'nn': 0.1}\n",
    "\n",
    "final_pred = (lgb_model.y_pred * weights['lbg']) + (xgb_model.y_pred * weights['xgb']) + (nn_model.y_pred * weights['nn'] + (3*class_pred * weights['cls']))\n",
    "#final_pred = cnn_model.y_pred\n",
    "# final_pred = (lgb_model.y_pred * weights['lbg']) + (xgb_model.y_pred * weights['xgb']) + (cat_model.y_pred * weights['cat'])\n",
    "print(final_pred.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.4440512385729405, 1: 1.8443449155461984, 2: 2.095548181937612}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3    0.504\n",
       "0    0.236\n",
       "1    0.136\n",
       "2    0.124\n",
       "Name: accuracy_group, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = Counter(reduce_train['accuracy_group'])\n",
    "# dist[0] = 0.261\n",
    "# dist[1] = 0.116\n",
    "# dist[2] = 0.119\n",
    "# dist[3] = 0.504\n",
    "for k in dist:\n",
    "    dist[k] /= len(reduce_train)\n",
    "reduce_train['accuracy_group'].hist()\n",
    "\n",
    "acum = 0\n",
    "bound = {}\n",
    "for i in range(3):\n",
    "    acum += dist[i]\n",
    "    bound[i] = np.percentile(final_pred, acum * 100)\n",
    "print(bound)\n",
    "def classify(x):\n",
    "    if x <= bound[0]:\n",
    "        return 0\n",
    "    elif x <= bound[1]:\n",
    "        return 1\n",
    "    elif x <= bound[2]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "final_pred = np.array(list(map(classify, final_pred)))\n",
    "\n",
    "sample_submission['accuracy_group'] = final_pred.astype(int)\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission['accuracy_group'].value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0fffe8610a3a47ce96406e5d3d46ae97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "259239529e394a228c82dee2123dded4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f83561d4e5f4fceb5e44eadd7da67ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0fffe8610a3a47ce96406e5d3d46ae97",
       "max": 17000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c790aaf9f11246dd834f028bed2714a2",
       "value": 17000
      }
     },
     "4b31b6d385ed42079ffb196029d1cb04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_885fed8f3523447b91b363bcde61518b",
       "max": 1000,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7629ed8d38f2457abe3a77d16d7e0432",
       "value": 1000
      }
     },
     "66b1112ac4e648c39ef4d7ed1fcdf8ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6a04052ef3314a6d9a7baa8d04975aec",
       "placeholder": "​",
       "style": "IPY_MODEL_704c747113734a579a0f3a642f63b90c",
       "value": " 17000/17000 [15:20&lt;00:00, 18.46it/s]"
      }
     },
     "6a04052ef3314a6d9a7baa8d04975aec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "704c747113734a579a0f3a642f63b90c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7629ed8d38f2457abe3a77d16d7e0432": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "7e80e368dc6741ebaa5a4212e757fff9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4b31b6d385ed42079ffb196029d1cb04",
        "IPY_MODEL_95dbd6476c754589997d68bceb9ee4c2"
       ],
       "layout": "IPY_MODEL_259239529e394a228c82dee2123dded4"
      }
     },
     "811cadae6915415aa711a29cdaddb614": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "885fed8f3523447b91b363bcde61518b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8edfc81f78174655b3659d1f1094757b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "95dbd6476c754589997d68bceb9ee4c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_99fea93375fb4d39b9627803d5e2ed4b",
       "placeholder": "​",
       "style": "IPY_MODEL_8edfc81f78174655b3659d1f1094757b",
       "value": " 1000/1000 [01:54&lt;00:00,  8.75it/s]"
      }
     },
     "99fea93375fb4d39b9627803d5e2ed4b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4c41fc94930401bab147a0389808cac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2f83561d4e5f4fceb5e44eadd7da67ae",
        "IPY_MODEL_66b1112ac4e648c39ef4d7ed1fcdf8ef"
       ],
       "layout": "IPY_MODEL_811cadae6915415aa711a29cdaddb614"
      }
     },
     "c790aaf9f11246dd834f028bed2714a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
